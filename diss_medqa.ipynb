{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9012001,"sourceType":"datasetVersion","datasetId":5429851}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/trduc97/neural_medical_qa.git\n    \n%cd neural_medical_qa\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:44:54.494349Z","iopub.execute_input":"2024-07-26T19:44:54.494715Z","iopub.status.idle":"2024-07-26T19:45:09.983514Z","shell.execute_reply.started":"2024-07-26T19:44:54.494685Z","shell.execute_reply":"2024-07-26T19:45:09.982362Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'neural_medical_qa'...\nremote: Enumerating objects: 76, done.\u001b[K\nremote: Counting objects: 100% (76/76), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 76 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (76/76), 1.75 MiB | 3.84 MiB/s, done.\n/kaggle/working/neural_medical_qa\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.20.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.19.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets->-r requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets->-r requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:45:09.985334Z","iopub.execute_input":"2024-07-26T19:45:09.985649Z","iopub.status.idle":"2024-07-26T19:45:22.100283Z","shell.execute_reply.started":"2024-07-26T19:45:09.985622Z","shell.execute_reply":"2024-07-26T19:45:22.099070Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 0. Dataset","metadata":{}},{"cell_type":"code","source":"from import_datasets import load_bioasq_pubmedqa,  train_val_test_split\n\nbioasq, pubmedqa = load_bioasq_pubmedqa()\n\n# Display the first few samples of the PubMedQA dataset\nprint(pubmedqa['train'].to_pandas().head())\n\nresponses = pubmedqa['train']['final_decision']\n# Counting the occurrences of each value\nyes_count = responses.count('yes')\nno_count = responses.count('no')\nmaybe_count = responses.count('maybe')\n\n# Display the counts\nprint(f\"Yes: {yes_count}\")\nprint(f\"No: {no_count}\")\nprint(f\"Maybe: {maybe_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:45:22.101793Z","iopub.execute_input":"2024-07-26T19:45:22.102160Z","iopub.status.idle":"2024-07-26T19:45:29.816915Z","shell.execute_reply.started":"2024-07-26T19:45:22.102129Z","shell.execute_reply":"2024-07-26T19:45:29.816002Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/5.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f475bca201e04d0396f3583e856b8789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85a30b522df4b0b944b057769913e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed0c7a1b7304ed4be199575b436749b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e40d8ab90f2421397718b9f58a679db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84432828d7374d7886b838c8007b251e"}},"metadata":{}},{"name":"stdout","text":"      pubid                                           question  \\\n0  21645374  Do mitochondria play a role in remodelling lac...   \n1  16418930  Landolt C and snellen e acuity: differences in...   \n2   9488747  Syncope during bathing in infants, a pediatric...   \n3  17208539  Are the long-term results of the transanal pul...   \n4  10808977  Can tailored interventions increase mammograph...   \n\n                                             context  \\\n0  {'contexts': ['Programmed cell death (PCD) is ...   \n1  {'contexts': ['Assessment of visual acuity dep...   \n2  {'contexts': ['Apparent life-threatening event...   \n3  {'contexts': ['The transanal endorectal pull-t...   \n4  {'contexts': ['Telephone counseling and tailor...   \n\n                                         long_answer final_decision  \\\n0  Results depicted mitochondrial dynamics in viv...            yes   \n1  Using the charts described, there was only a s...             no   \n2  \"Aquagenic maladies\" could be a pediatric form...            yes   \n3  Our long-term study showed significantly bette...             no   \n4  The effects of the intervention were most pron...            yes   \n\n   decision_encoded  \n0                 2  \n1                 0  \n2                 2  \n3                 0  \n4                 2  \nYes: 552\nNo: 338\nMaybe: 110\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Split the dataset","metadata":{}},{"cell_type":"code","source":"pubmedqa_train,pubmedqa_val, pubmedqa_test = train_val_test_split(pubmedqa)\nprint(f\"Train size: {len(pubmedqa_train)}\")\nprint(f\"Validation size: {len(pubmedqa_val)}\")\nprint(f\"Test size: {len(pubmedqa_test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:45:29.819134Z","iopub.execute_input":"2024-07-26T19:45:29.819773Z","iopub.status.idle":"2024-07-26T19:45:30.073554Z","shell.execute_reply.started":"2024-07-26T19:45:29.819739Z","shell.execute_reply":"2024-07-26T19:45:30.072550Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train size: 750\nValidation size: 100\nTest size: 150\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n# Initialize a defaultdict to hold the bucket counts\nlength_buckets = defaultdict(int)\n\n# Define the bucket size\nbucket_size = 128\n\n# Loop through each string in the list\nfor s in pubmedqa_train['long_answer']:\n    # Determine the bucket for the current string length\n    bucket = (len(s) // bucket_size) * bucket_size\n    # Increment the count for the appropriate bucket\n    length_buckets[bucket] += 1\n\n# Display the counts for each bucket\nfor bucket, count in sorted(length_buckets.items()):\n    print(f\"Length {bucket} - {bucket + bucket_size - 1}: {count} strings\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:45:30.074947Z","iopub.execute_input":"2024-07-26T19:45:30.075377Z","iopub.status.idle":"2024-07-26T19:45:30.084374Z","shell.execute_reply.started":"2024-07-26T19:45:30.075330Z","shell.execute_reply":"2024-07-26T19:45:30.083519Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Length 0 - 127: 66 strings\nLength 128 - 255: 326 strings\nLength 256 - 383: 244 strings\nLength 384 - 511: 82 strings\nLength 512 - 639: 25 strings\nLength 640 - 767: 4 strings\nLength 768 - 895: 3 strings\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Model","metadata":{}},{"cell_type":"markdown","source":"## 3.0. Importing libraries","metadata":{}},{"cell_type":"code","source":"# @title\nimport pickle\nfrom sklearn.metrics import roc_auc_score\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom tensorflow.keras.layers import GlobalMaxPool2D, GlobalAvgPool2D, Multiply, Subtract, Add, Activation, MaxPool2D, Concatenate,Reshape, Conv2D, MaxPooling2D, Dropout, Input, Embedding, Bidirectional, LSTM, Dense, Lambda, GlobalAveragePooling1D, Reshape, GRU, Flatten, Dropout, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.activations import gelu\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport zipfile\nimport os\nimport tensorflow as tf\n# Setting epoch_count = 1 just to let the file run as demo, we have saved the trained models in a separated folder and will be using this to review\nbatch_size=256\nepoch_count=10\nactivation_func=gelu","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:46:08.432524Z","iopub.execute_input":"2024-07-26T19:46:08.432926Z","iopub.status.idle":"2024-07-26T19:46:19.715364Z","shell.execute_reply.started":"2024-07-26T19:46:08.432891Z","shell.execute_reply":"2024-07-26T19:46:19.714219Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-07-26 19:46:10.174335: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-26 19:46:10.174441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-26 19:46:10.308868: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# List all available devices\nphysical_devices = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(physical_devices))\n\nresults={}","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:46:19.717485Z","iopub.execute_input":"2024-07-26T19:46:19.718307Z","iopub.status.idle":"2024-07-26T19:46:19.889806Z","shell.execute_reply.started":"2024-07-26T19:46:19.718272Z","shell.execute_reply":"2024-07-26T19:46:19.888682Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.utils import class_weight\nimport os\nimport zipfile\n\nclass ModelTraining:\n    def __init__(self, responses):\n        # Prepare a dictionary to record result\n        self.dl_results={}\n        # Rebalanced classes weights\n        self.class_weights = self.compute_class_weights(responses)\n        # Define metrics\n        self.metrics = self.define_metrics()\n        self._optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5)\n        self._loss_func=tf.keras.losses.CategoricalCrossentropy()\n        self.histories={}\n\n    # metrics\n    def f1_score_metrics(self, y_true, y_pred):\n        true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n        possible_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n        predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n        recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        return f1\n    \n    def define_metrics(self):\n        return [\n            tf.keras.metrics.Accuracy(name='accuracy'),\n#            tf.keras.metrics.Precision(name='precision'),\n#            tf.keras.metrics.Recall(name='recall'),\n#            tf.keras.metrics.AUC(name='auc'),\n#            tf.keras.metrics.F1Score\n#            self.f1_score_metrics,\n        ]\n    #Calculate class weights\n    def compute_class_weights(self, responses):\n        class_weights = class_weight.compute_class_weight(class_weight='balanced',\n                                                          classes=np.unique(responses),\n                                                          y=responses)\n        # Convert class weights to a dictionary\n        return dict(enumerate(class_weights))\n\n    # Setting up function to measure the models performance\n    def dl_metrics(self, test_dataset, model, model_name):\n        test_results = model.evaluate(test_dataset)\n        self.dl_results[model_name] = test_results\n\n    # Code to zip\n    def zip_folder(self, folder_path, zip_path):\n        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            # Walk through all the files in the folder\n            for root, dirs, files in os.walk(folder_path):\n                for file in files:\n                    # Add each file to the zip file\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n    def compile_model(self,model, model_name):\n        model.compile(optimizer=self._optimizer,\n                      loss=self._loss_func,\n                      metrics=self.metrics)\n#        model.summary()\n#        model_plot = model_name +'.png'\n#        plot_model(model,show_shapes=True,to_file = model_plot)\n    # Code to train model\n    def train_model(self, model,model_name,\n                    train_dataset,\n                    validate_dataset,\n                    test_dataset,\n                    epochs=epoch_count, batch_size=batch_size):\n\n        history = model.fit(train_dataset,\n                            epochs=epochs,\n                            #batch_size=batch_size,\n                            validation_data=validate_dataset,\n#                            class_weight=self.class_weights,\n                            )\n        self.histories[model_name] = history\n\n        # measure performance on the test dataset and save to dicitonary\n\n        self.dl_metrics(test_dataset, model, model_name)\n        # Saving the trained model\n        model.save(model_name)\n\n        # Specify the folder to be zipped and the path to save the zip file\n        zip_file_path = model_name + '.zip'\n\n        # Call the function to zip the folder\n        self.zip_folder(model_name, zip_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:51:25.863874Z","iopub.execute_input":"2024-07-26T19:51:25.864272Z","iopub.status.idle":"2024-07-26T19:51:25.882059Z","shell.execute_reply.started":"2024-07-26T19:51:25.864244Z","shell.execute_reply":"2024-07-26T19:51:25.881094Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Text representation","metadata":{}},{"cell_type":"markdown","source":"### 3.1.1. TF-IDF","metadata":{}},{"cell_type":"markdown","source":"### 3.1.2. Embedding layer","metadata":{}},{"cell_type":"markdown","source":"### 3.1.3. ELMo ","metadata":{}},{"cell_type":"markdown","source":"### 3.1.4. BERT (base-uncased)","metadata":{}},{"cell_type":"code","source":"embedding='bert'\nneural_model='dense'\nmodel_name=embedding+'_'+neural_model","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:46:56.707960Z","iopub.execute_input":"2024-07-26T19:46:56.708808Z","iopub.status.idle":"2024-07-26T19:46:56.712865Z","shell.execute_reply.started":"2024-07-26T19:46:56.708781Z","shell.execute_reply":"2024-07-26T19:46:56.711934Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pubmedqa_train","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:54:16.348264Z","iopub.execute_input":"2024-07-26T19:54:16.349178Z","iopub.status.idle":"2024-07-26T19:54:16.355736Z","shell.execute_reply.started":"2024-07-26T19:54:16.349147Z","shell.execute_reply":"2024-07-26T19:54:16.354751Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision', 'decision_encoded', '__index_level_0__'],\n    num_rows: 750\n})"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom datasets import load_dataset\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_function(examples):\n    return tokenizer(examples['question'], examples['long_answer'], truncation=True, padding='max_length', max_length=512)\n\n# Tokenize the datasets\ntrain_dataset = pubmedqa_train.map(preprocess_function, batched=True)\nval_dataset = pubmedqa_val.map(preprocess_function, batched=True)\ntest_dataset = pubmedqa_test.map(preprocess_function, batched=True)\n\n# Convert the datasets to TensorFlow datasets\ndef convert_to_tf_dataset(dataset, shuffle=True, batch_size=32):\n    def gen():\n        for ex in dataset:\n            yield ({'input_ids': ex['input_ids'], 'attention_mask': ex['attention_mask']}, ex['decision_encoded'])\n    \n    return tf.data.Dataset.from_generator(gen, \n                                          ({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int64),\n                                          ({'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None])}, tf.TensorShape([]))\n                                         ).shuffle(1000).batch(batch_size)\n\ntrain_tf_dataset = convert_to_tf_dataset(train_dataset)\nval_tf_dataset = convert_to_tf_dataset(val_dataset, shuffle=False)\ntest_tf_dataset = convert_to_tf_dataset(test_dataset, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:46:59.050224Z","iopub.execute_input":"2024-07-26T19:46:59.050595Z","iopub.status.idle":"2024-07-26T19:47:09.978375Z","shell.execute_reply.started":"2024-07-26T19:46:59.050562Z","shell.execute_reply":"2024-07-26T19:47:09.977386Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbdb856d16aa414494749343f2d3053f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e919c407b79648348aa20b40afbb3596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a102d9598e4fa7b215861e57f6cfdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3522cdefd9ed443985998c247fb36105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/750 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649988bc782f4f5492cc3d862d8529b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4aeb21f62b446aa65345df09c2d135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af0a3c61d4814d2db68e99cc5321900b"}},"metadata":{}}]},{"cell_type":"code","source":"# Instantiate a model training session\nmodel_training = ModelTraining(responses)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:51:31.363266Z","iopub.execute_input":"2024-07-26T19:51:31.364020Z","iopub.status.idle":"2024-07-26T19:51:31.374024Z","shell.execute_reply.started":"2024-07-26T19:51:31.363988Z","shell.execute_reply":"2024-07-26T19:51:31.373181Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class PubMedQAModel(tf.keras.Model):\n    def __init__(self, model_name='bert-base-uncased',classes=3, dropout=0.3,nodes=32):\n        super(PubMedQAModel, self).__init__()\n        self.bert = TFBertModel.from_pretrained(model_name)      \n        # Freeze all layers except the last two\n        for layer in self.bert.layers:\n            layer.trainable = False\n        for layer in self.bert.bert.encoder.layer[-2:]:\n            layer.trainable = True\n        self.dropout1 = tf.keras.layers.Dropout(dropout)\n        self.dropout2 = tf.keras.layers.Dropout(dropout)\n        self.dense1 = tf.keras.layers.Dense(nodes, activation='relu')\n        self.classifier = tf.keras.layers.Dense(classes, activation='softmax')\n    \n    def call(self, inputs):\n        input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        dropout_output1 = self.dropout1(pooled_output, training=False)\n        dense_output1= self.dense1(dropout_output1)\n        dropout_output2 = self.dropout2(dense_output1, training=False)        \n        logits = self.classifier(dropout_output2)\n        return logits\n\nmodel = PubMedQAModel()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:51:33.174157Z","iopub.execute_input":"2024-07-26T19:51:33.174529Z","iopub.status.idle":"2024-07-26T19:51:34.667087Z","shell.execute_reply.started":"2024-07-26T19:51:33.174500Z","shell.execute_reply":"2024-07-26T19:51:34.666151Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_training.compile_model(model, model_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:51:37.443491Z","iopub.execute_input":"2024-07-26T19:51:37.443868Z","iopub.status.idle":"2024-07-26T19:51:37.452412Z","shell.execute_reply.started":"2024-07-26T19:51:37.443829Z","shell.execute_reply":"2024-07-26T19:51:37.450512Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model_training.train_model(model, model_name,\n                            train_tf_dataset, \n#                            val_tf_dataset,\n                            test_tf_dataset\n                           )","metadata":{"execution":{"iopub.status.busy":"2024-07-26T19:51:39.206046Z","iopub.execute_input":"2024-07-26T19:51:39.206914Z","iopub.status.idle":"2024-07-26T19:51:43.791567Z","shell.execute_reply.started":"2024-07-26T19:51:39.206880Z","shell.execute_reply":"2024-07-26T19:51:43.790162Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_tf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;43;03m#                            val_tf_dataset,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtest_tf_dataset\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 76\u001b[0m, in \u001b[0;36mModelTraining.train_model\u001b[0;34m(self, model, model_name, train_dataset, test_dataset, epochs, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model,model_name,\n\u001b[1;32m     71\u001b[0m                     train_dataset,\n\u001b[1;32m     72\u001b[0m  \u001b[38;5;66;03m#                   validate_dataset,\u001b[39;00m\n\u001b[1;32m     73\u001b[0m                     test_dataset,\n\u001b[1;32m     74\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepoch_count, batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m---> 76\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m#batch_size=batch_size,\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;43;03m#                            validation_data=validate_dataset,\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;43;03m#                            class_weight=self.class_weights,\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistories[model_name] \u001b[38;5;241m=\u001b[39m history\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# measure performance on the test dataset and save to dicitonary\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:554\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m     )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n","\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 3)"],"ename":"ValueError","evalue":"Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 3)","output_type":"error"}]},{"cell_type":"code","source":"for data, label in train_tf_dataset.take(1):\n    print(data)\n    print(label)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:47:37.966231Z","iopub.execute_input":"2024-07-25T22:47:37.967119Z","iopub.status.idle":"2024-07-25T22:47:39.193936Z","shell.execute_reply.started":"2024-07-25T22:47:37.967084Z","shell.execute_reply":"2024-07-25T22:47:39.193000Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"{'input_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\narray([[  101, 26351, 17635, ...,     0,     0,     0],\n       [  101,  2003,  1996, ...,     0,     0,     0],\n       [  101,  2041,  1997, ...,     0,     0,     0],\n       ...,\n       [  101,  2003,  4367, ...,     0,     0,     0],\n       [  101,  2566, 12690, ...,     0,     0,     0],\n       [  101,  2515,  1996, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\ntf.Tensor([2 2 2 0 0 0 2 0 0 2 2 2 2 0 2 2 0 1 2 1 1 2 0 2 2 2 2 0 0 2 0 2], shape=(32,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\nmodel.fit(train_tf_dataset, validation_data=val_tf_dataset, epochs=15)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:58:42.416031Z","iopub.execute_input":"2024-07-25T22:58:42.416396Z","iopub.status.idle":"2024-07-25T23:02:52.896529Z","shell.execute_reply.started":"2024-07-25T22:58:42.416367Z","shell.execute_reply":"2024-07-25T23:02:52.895539Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.4777 - loss: 1.0629 - val_accuracy: 0.5200 - val_loss: 1.0243\nEpoch 2/15\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1721948363.754266     144 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 519ms/step - accuracy: 0.5637 - loss: 0.9935 - val_accuracy: 0.5400 - val_loss: 0.9955\nEpoch 3/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5492 - loss: 0.9814 - val_accuracy: 0.5400 - val_loss: 0.9806\nEpoch 4/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5608 - loss: 0.9580 - val_accuracy: 0.5400 - val_loss: 0.9726\nEpoch 5/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5552 - loss: 0.9546 - val_accuracy: 0.5400 - val_loss: 0.9676\nEpoch 6/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 520ms/step - accuracy: 0.5488 - loss: 0.9454 - val_accuracy: 0.5400 - val_loss: 0.9637\nEpoch 7/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5602 - loss: 0.9408 - val_accuracy: 0.5400 - val_loss: 0.9604\nEpoch 8/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5380 - loss: 0.9550 - val_accuracy: 0.5400 - val_loss: 0.9581\nEpoch 9/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5542 - loss: 0.9365 - val_accuracy: 0.5400 - val_loss: 0.9574\nEpoch 10/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5421 - loss: 0.9489 - val_accuracy: 0.5400 - val_loss: 0.9553\nEpoch 11/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5457 - loss: 0.9367 - val_accuracy: 0.5400 - val_loss: 0.9542\nEpoch 12/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5398 - loss: 0.9486 - val_accuracy: 0.5400 - val_loss: 0.9531\nEpoch 13/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5506 - loss: 0.9383 - val_accuracy: 0.5400 - val_loss: 0.9527\nEpoch 14/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 519ms/step - accuracy: 0.5405 - loss: 0.9443 - val_accuracy: 0.5400 - val_loss: 0.9515\nEpoch 15/15\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 519ms/step - accuracy: 0.5379 - loss: 0.9364 - val_accuracy: 0.5400 - val_loss: 0.9503\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d894ee83f70>"},"metadata":{}}]},{"cell_type":"code","source":"model.evaluate(test_tf_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T23:02:52.898046Z","iopub.execute_input":"2024-07-25T23:02:52.898348Z","iopub.status.idle":"2024-07-25T23:02:58.010430Z","shell.execute_reply.started":"2024-07-25T23:02:52.898321Z","shell.execute_reply":"2024-07-25T23:02:58.009574Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"      4/Unknown \u001b[1m2s\u001b[0m 460ms/step - accuracy: 0.5358 - loss: 0.9711","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1721948577.265502     143 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 996ms/step - accuracy: 0.5439 - loss: 0.9633\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[0.9476428031921387, 0.5600000023841858]"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel\n\nclass PubMedQAModel(tf.keras.Model):\n    def __init__(self, model_name='bert-base-uncased',classes=3, dropout=0.3,nodes=32):\n        super(PubMedQAModel, self).__init__()\n        self.bert = TFBertModel.from_pretrained(model_name)      \n        # Freeze all layers except the last two\n        for layer in self.bert.layers:\n            layer.trainable = False\n#        for layer in self.bert.bert.encoder.layer[-2:]:\n#            layer.trainable = True\n        self.dropout1 = tf.keras.layers.Dropout(dropout)\n#        self.dropout2 = tf.keras.layers.Dropout(dropout)\n#        self.dense1 = tf.keras.layers.Dense(nodes, activation='relu')\n        self.classifier = tf.keras.layers.Dense(classes, activation='softmax')\n    \n    def call(self, inputs):\n        input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        dropout_output1 = self.dropout1(pooled_output, training=False)\n#        dense_output1= self.dense1(dropout_output1)\n#        dropout_output2 = self.dropout2(dense_output1, training=False)        \n        logits = self.classifier(dropout_output1)\n        return logits\n\nmodel = PubMedQAModel()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T23:24:52.323142Z","iopub.execute_input":"2024-07-25T23:24:52.323912Z","iopub.status.idle":"2024-07-25T23:24:53.831727Z","shell.execute_reply.started":"2024-07-25T23:24:52.323879Z","shell.execute_reply":"2024-07-25T23:24:53.830803Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\n# Train the model\nepochs = 10\n\nhistory = model.fit(train_tf_dataset, validation_data=val_tf_dataset, epochs=epochs)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(test_tf_dataset)\nprint(f'Test Loss: {loss}')\nprint(f'Test Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T23:24:56.382152Z","iopub.execute_input":"2024-07-25T23:24:56.382887Z","iopub.status.idle":"2024-07-25T23:27:56.108090Z","shell.execute_reply.started":"2024-07-25T23:24:56.382853Z","shell.execute_reply":"2024-07-25T23:27:56.107097Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Epoch 1/10\n     24/Unknown \u001b[1m35s\u001b[0m 614ms/step - accuracy: 0.1213 - loss: 1.2338","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.1219 - loss: 1.2330 - val_accuracy: 0.2000 - val_loss: 1.1472\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1721949941.069179     146 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.2068 - loss: 1.1463 - val_accuracy: 0.4400 - val_loss: 1.0744\nEpoch 3/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.4435 - loss: 1.0702 - val_accuracy: 0.4800 - val_loss: 1.0267\nEpoch 4/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.5112 - loss: 1.0278 - val_accuracy: 0.5100 - val_loss: 0.9955\nEpoch 5/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 521ms/step - accuracy: 0.5246 - loss: 0.9928 - val_accuracy: 0.5500 - val_loss: 0.9761\nEpoch 6/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.5306 - loss: 0.9818 - val_accuracy: 0.5500 - val_loss: 0.9637\nEpoch 7/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.5588 - loss: 0.9506 - val_accuracy: 0.5500 - val_loss: 0.9553\nEpoch 8/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 522ms/step - accuracy: 0.5315 - loss: 0.9579 - val_accuracy: 0.5500 - val_loss: 0.9509\nEpoch 9/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 522ms/step - accuracy: 0.5692 - loss: 0.9340 - val_accuracy: 0.5500 - val_loss: 0.9479\nEpoch 10/10\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 521ms/step - accuracy: 0.5554 - loss: 0.9451 - val_accuracy: 0.5500 - val_loss: 0.9456\n      4/Unknown \u001b[1m2s\u001b[0m 461ms/step - accuracy: 0.5716 - loss: 0.9565","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1721950075.699842     143 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5611 - loss: 0.9593\nTest Loss: 0.9649049043655396\nTest Accuracy: 0.5400000214576721\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T23:20:31.631802Z","iopub.execute_input":"2024-07-25T23:20:31.632164Z","iopub.status.idle":"2024-07-25T23:20:31.652448Z","shell.execute_reply.started":"2024-07-25T23:20:31.632135Z","shell.execute_reply":"2024-07-25T23:20:31.651592Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"pub_med_qa_model_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pub_med_qa_model_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m24,608\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m74,123\u001b[0m (289.55 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,123</span> (289.55 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,707\u001b[0m (96.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,707</span> (96.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m49,416\u001b[0m (193.04 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,416</span> (193.04 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3.1.5. PubMedBert","metadata":{}},{"cell_type":"markdown","source":"## 3.2. Models ","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1. Match-LSTM","metadata":{}},{"cell_type":"markdown","source":"### 3.2.2. BiDAF","metadata":{}},{"cell_type":"markdown","source":"# 4. Evaluate the results","metadata":{}}]}