{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9012001,"sourceType":"datasetVersion","datasetId":5429851},{"sourceId":9062110,"sourceType":"datasetVersion","datasetId":5465014}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import shutil\nimport os\n\n# Remove the directory if already exist \ndir_name = 'neural_medical_qa'\nif os.path.exists(dir_name):\n    shutil.rmtree(dir_name)\n\n#clone the repo from github\n!git clone https://github.com/trduc97/neural_medical_qa.git\n%cd neural_medical_qa\n# install the requirement\n!pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from import_datasets import load_bioasq_pubmedqa,  train_val_test_split\n\nbioasq, pubmedqa = load_bioasq_pubmedqa()\n\n# Display the first few samples of the PubMedQA dataset\nprint(pubmedqa['train'].to_pandas().head())\n\nresponses = pubmedqa['train']['final_decision']\n# Counting the occurrences of each value\nyes_count = responses.count('yes')\nno_count = responses.count('no')\nmaybe_count = responses.count('maybe')\n\n# Display the counts\nprint(f\"Yes: {yes_count}\")\nprint(f\"No: {no_count}\")\nprint(f\"Maybe: {maybe_count}\")\n\npubmedqa_train,pubmedqa_val, pubmedqa_test = train_val_test_split(pubmedqa)\nprint(f\"Train size: {len(pubmedqa_train)}\")\nprint(f\"Validation size: {len(pubmedqa_val)}\")\nprint(f\"Test size: {len(pubmedqa_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n# Initialize a defaultdict to hold the bucket counts\nlength_buckets = defaultdict(int)\n\n# Define the bucket size\nbucket_size = 128\n\n# Loop through each string in the list\nfor s in pubmedqa_train['long_answer']:\n    # Determine the bucket for the current string length\n    bucket = (len(s) // bucket_size) * bucket_size\n    # Increment the count for the appropriate bucket\n    length_buckets[bucket] += 1\n\n# Display the counts for each bucket\nfor bucket, count in sorted(length_buckets.items()):\n    print(f\"Length {bucket} - {bucket + bucket_size - 1}: {count} strings\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bioasq, pubmedqa_artificial = load_bioasq_pubmedqa(pubmed_kaggle_path='/kaggle/input/pubmed-qa/pubmed_qa_pga_artificial.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\nfrom sklearn.model_selection import train_test_split\n\ndf_artificial=pubmedqa_artificial['train'].to_pandas()\ndf_sample, _=train_test_split(df_artificial, test_size=0.95, random_state=42, stratify=df_artificial['decision_encoded'])   \ndf_sample=df_sample[['pubid', 'question', 'context', 'long_answer', 'final_decision', 'decision_encoded']]\ndata_art=Dataset.from_pandas(df_sample,preserve_index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert back to datasets\npubmedqa_arti = DatasetDict({'train': data_art})\npubmedqa_art_train,pubmedqa_art_val, pubmedqa_art_test = train_val_test_split(pubmedqa_arti)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2Model\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n\nclass QAModel(nn.Module):\n    def __init__(self, model, classes=3, dropout_prob=0.5):\n        super(QAModel, self).__init__()\n        self.bert = model\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.linear1 = nn.Linear(model.config.hidden_size, 128)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.linear2 = nn.Linear(128, classes)  # number of classes may vary between BioASQ (2 classes) and PubMedQA (3 classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n        cls_output = self.dropout1(cls_output)  # Apply first dropout\n        cls_output = self.linear1(cls_output)  # Apply first linear layer\n        cls_output = self.dropout2(cls_output)  # Apply second dropout\n        logits = self.linear2(cls_output)  # Apply second linear layer\n        return logits\n\nclass TrainandValidate:\n\n    def __init__(self, model_name, source, df_train, df_val, df_test, stratify_col='decision_encoded'):\n        self.name = model_name\n        self.source = source\n        self.batch_size = 16 if 'GPT' in self.name or 'artificial' in self.name else 64\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = self.initialize_tokenizer()\n        self.stratify_col = stratify_col\n\n        self.train_inputs, self.train_labels = self.encode_data(df_train)\n        self.validate_inputs, self.validate_labels = self.encode_data(df_val)\n        self.test_inputs, self.test_labels = self.encode_data(df_test)\n\n        self.train_loader = self.create_dataloader(self.train_inputs, self.train_labels)\n        self.validate_loader = self.create_dataloader(self.validate_inputs, self.validate_labels)\n        self.test_loader = self.create_dataloader(self.test_inputs, self.test_labels)\n\n        self.model = self.create_model().to(self.device)  \n        self.optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def initialize_tokenizer(self):\n        if 'GPT' in self.name:\n            tokenizer = GPT2Tokenizer.from_pretrained(self.source)\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n            return tokenizer\n        elif 'BioLinkBERT' in self.name:\n            return AutoTokenizer.from_pretrained(self.source)\n        else:\n            return BertTokenizer.from_pretrained(self.source)\n\n    def encode_data(self, df):\n        inputs = self.tokenizer(\n            text=df['question'], \n            text_pair=df['long_answer'], \n            padding=True, \n            truncation=True, \n            return_tensors='pt', \n            max_length=128*4\n        )\n        labels = torch.tensor(df[self.stratify_col])\n        return inputs, labels\n\n    def create_dataloader(self, inputs, labels):\n        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n    def create_model(self):\n        if 'GPT' in self.name:\n            model = GPT2Model.from_pretrained(self.source)\n            model.resize_token_embeddings(len(self.tokenizer))\n            model = QAModel(model)\n        elif 'BioLinkBERT' in self.name:\n            model = AutoModel.from_pretrained(self.source)\n            model = QAModel(model)\n        else:\n            model = BertModel.from_pretrained(self.source)\n            model = QAModel(model)\n        return model\n\n    def calculate_f1_score(self, preds, labels):\n        preds_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return f1_score(labels_flat, preds_flat, average='weighted')\n\n    def evaluate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        predictions, true_labels = [], []\n    \n        with torch.no_grad():\n            for batch in dataloader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                outputs = self.model(b_input_ids, b_attention_mask)\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                predictions.extend(np.argmax(logits, axis=1))\n                true_labels.extend(label_ids)\n    \n        accuracy = accuracy_score(true_labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n    \n        return accuracy, precision, recall, f1\n\n    def training(self, epochs=15):\n        self.model.train()\n        epochs = 5 if 'artificial' in self.name else epochs\n        for epoch in range(epochs):\n            total_loss = 0\n            all_preds = []\n            all_labels = []\n        \n            for batch in self.train_loader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                self.optimizer.zero_grad()\n            \n                outputs = self.model(b_input_ids, b_attention_mask)\n                loss = self.loss_fn(outputs, b_labels)\n                loss.backward()\n                self.optimizer.step()\n            \n                total_loss += loss.item()\n            \n                preds = outputs.detach().cpu().numpy()\n                label_ids = b_labels.to('cpu').numpy()\n            \n                all_preds.append(preds)\n                all_labels.append(label_ids)\n        \n            avg_loss = total_loss / len(self.train_loader)\n            all_preds = np.concatenate(all_preds, axis=0)\n            all_labels = np.concatenate(all_labels, axis=0)\n            avg_f1_score = self.calculate_f1_score(all_preds, all_labels)\n        \n            print(f\"Epoch {epoch+1}, Loss: {avg_loss}, F1 Score: {avg_f1_score}\")\n        \n        self.save_model()\n\n    def save_model(self):\n        os.makedirs('/kaggle/working/models', exist_ok=True)\n        model_path = f'/kaggle/working/models/{self.name}_model.pth'\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, model_path)\n        print(f\"Model saved to {model_path}\")\n\n    def load_model(self, model_path):\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(f\"Model loaded from {model_path}\")\n\n    def val(self, load_model_path=None):\n        if load_model_path:\n            self.load_model(load_model_path)\n        \n        val_accuracy, val_precision, val_recall, val_f1 = self.evaluate(self.validate_loader)\n        print(f\"Validation - Accuracy: {val_accuracy}, Precision: {val_precision}, Recall: {val_recall}, F1-Score: {val_f1}\")\n        \n        test_accuracy, test_precision, test_recall, test_f1 = self.evaluate(self.test_loader)\n        print(f\"Test - Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1-Score: {test_f1}\")\n    \n        return {\n            'validation': {\n                'accuracy': val_accuracy,\n                'precision': val_precision,\n                'recall': val_recall,\n                'f1': val_f1\n            },\n            'test': {\n                'accuracy': test_accuracy,\n                'precision': test_precision,\n                'recall': test_recall,\n                'f1': test_f1\n            }\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [\n    {\n        'model_name': 'BERT',\n        'source': 'bert-base-uncased',\n        'df_train': pubmedqa_train,\n        'df_val': pubmedqa_val,\n        'df_test': pubmedqa_test\n    },\n    {\n        'model_name': 'BioLinkBERT',\n        'source': 'michiyasunaga/BioLinkBERT-base',\n        'df_train': pubmedqa_train,\n        'df_val': pubmedqa_val,\n        'df_test': pubmedqa_test\n    },\n    {\n        'model_name': 'GPT',\n        'source': 'gpt2',\n        'df_train': pubmedqa_train,\n        'df_val': pubmedqa_val,\n        'df_test': pubmedqa_test\n    },\n    {\n        'model_name': 'BiomedNLP',\n        'source': 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract',\n        'df_train': pubmedqa_train,\n        'df_val': pubmedqa_val,\n        'df_test': pubmedqa_test\n    }\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_results = {}\n\nfor model in models:\n    trainer = TrainandValidate(\n        model_name=model['model_name'],\n        source=model['source'],\n        df_train=model['df_train'],\n        df_val=model['df_val'],\n        df_test=model['df_test']\n    )   \n    # Train the model\n    trainer.training()\n    \n    # Validate the model\n    val_result = trainer.val()\n    val_results[model['model_name']] = val_result","metadata":{},"execution_count":null,"outputs":[]}]}