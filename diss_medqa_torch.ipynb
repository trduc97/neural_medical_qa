{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9012001,"sourceType":"datasetVersion","datasetId":5429851},{"sourceId":9062110,"sourceType":"datasetVersion","datasetId":5465014}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Importing","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Remove the directory if already exist \ndir_name = 'neural_medical_qa'\nif os.path.exists(dir_name):\n    shutil.rmtree(dir_name)\n\n#clone the repo from github\n!git clone https://github.com/trduc97/neural_medical_qa.git\n%cd neural_medical_qa\n# install the requirement\n!pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-05T19:23:38.587718Z","iopub.execute_input":"2024-08-05T19:23:38.588060Z","iopub.status.idle":"2024-08-05T19:23:52.579571Z","shell.execute_reply.started":"2024-08-05T19:23:38.588026Z","shell.execute_reply":"2024-08-05T19:23:52.578450Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'neural_medical_qa'...\nremote: Enumerating objects: 132, done.\u001b[K\nremote: Counting objects: 100% (132/132), done.\u001b[K\nremote: Compressing objects: 100% (125/125), done.\u001b[K\nremote: Total 132 (delta 62), reused 0 (delta 0), pack-reused 0\u001b[K\nReceiving objects: 100% (132/132), 2.28 MiB | 10.02 MiB/s, done.\nResolving deltas: 100% (62/62), done.\n/kaggle/working/neural_medical_qa/neural_medical_qa\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.20.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.19.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets->-r requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets->-r requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2. Importing data","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1. Importing pubmedqa and bioasq","metadata":{}},{"cell_type":"code","source":"from import_datasets import load_bioasq_pubmedqa, train_test_split\n\nbioasq, pubmedqa = load_bioasq_pubmedqa()\n\n# Display the first few samples of the PubMedQA dataset\nprint(pubmedqa['train'].to_pandas().head())\n\nresponses = pubmedqa['train']['final_decision']\n# Counting the occurrences of each value\nyes_count = responses.count('yes')\nno_count = responses.count('no')\nmaybe_count = responses.count('maybe')\n\n# Display the counts\nprint(f\"Yes: {yes_count}\")\nprint(f\"No: {no_count}\")\nprint(f\"Maybe: {maybe_count}\")\n\npubmedqa_train, pubmedqa_test = pubmed_train_test_split(pubmedqa)\nprint(f\"Train size: {len(pubmedqa_train)}\")\nprint(f\"Test size: {len(pubmedqa_test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:23:54.774695Z","iopub.execute_input":"2024-08-05T19:23:54.775185Z","iopub.status.idle":"2024-08-05T19:23:56.384634Z","shell.execute_reply.started":"2024-08-05T19:23:54.775155Z","shell.execute_reply":"2024-08-05T19:23:56.383720Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e5ce2d561b945e4b5de13b3b65061a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c99956973840039bf29a0d289023bf"}},"metadata":{}},{"name":"stdout","text":"      pubid                                           question  \\\n0  21645374  Do mitochondria play a role in remodelling lac...   \n1  16418930  Landolt C and snellen e acuity: differences in...   \n2   9488747  Syncope during bathing in infants, a pediatric...   \n3  17208539  Are the long-term results of the transanal pul...   \n4  10808977  Can tailored interventions increase mammograph...   \n\n                                             context  \\\n0  {'contexts': ['Programmed cell death (PCD) is ...   \n1  {'contexts': ['Assessment of visual acuity dep...   \n2  {'contexts': ['Apparent life-threatening event...   \n3  {'contexts': ['The transanal endorectal pull-t...   \n4  {'contexts': ['Telephone counseling and tailor...   \n\n                                         long_answer final_decision  \\\n0  Results depicted mitochondrial dynamics in viv...            yes   \n1  Using the charts described, there was only a s...             no   \n2  \"Aquagenic maladies\" could be a pediatric form...            yes   \n3  Our long-term study showed significantly bette...             no   \n4  The effects of the intervention were most pron...            yes   \n\n   decision_encoded  \n0                 2  \n1                 0  \n2                 2  \n3                 0  \n4                 2  \nYes: 552\nNo: 338\nMaybe: 110\nTrain size: 750\nTest size: 250\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndef load_bioasq_pubmedqa(bioasq_kaggle_path = '/kaggle/input/bioasq-training-12b/training12b_new.json', \n                         pubmed_kaggle_path='/kaggle/input/pubmed-qa/pubmed_qa_pga_labeled.parquet'):\n    # Load the JSON file\n    with open(bioasq_kaggle_path,'r') as f:\n        bioasq_data=json.load(f)\n    # Extract yes/no questions directly\n    bioasq_yesno = [{\n            'id':question['id'],\n            'question':question['body'],\n            'final_decision':question['exact_answer'],\n            'long_answer':question['ideal_answer'], \n            'documents':question['documents']\n        }\n        for question in bioasq_data['questions'] if question['type'] == 'yesno']\n    # Convert the list of yes/no questions to a Pandas DataFrame\n    bioasq_df = pd.DataFrame(bioasq_yesno)\n\n    # Convert the DataFrame to a Hugging Face Dataset\n    bioasq_dataset = Dataset.from_pandas(bioasq_df)\n    # Create a DatasetDict with the 'train' split\n    bioasq_data=DatasetDict({'train': bioasq_dataset})\n\n    # Read from parquet and translate to a dataset object\n    pubmed_df=pd.read_parquet(pubmed_kaggle_path)\n    dataset=Dataset.from_pandas(pubmed_df,preserve_index=False)\n    #Setting into similar format as from huggingface\n    pubmedqa_data = DatasetDict({'train': dataset})\n    \n    # Load the pubmedqa dataset\n    #pubmedqa_data=load_dataset(\"pubmed_qa\",\"pqa_labeled\") # unstable connection\n\n    #Encoding decisions \n    def decision_encode(question):\n        labels_map = {'no': 0, 'maybe': 1, 'yes': 2}\n        question['decision_encoded'] = labels_map[question['final_decision']]\n        return question\n\n    pubmedqa_data=pubmedqa_data.map(decision_encode)\n    bioasq_data=pubmedqa_data.map(decision_encode)\n\n    return bioasq_data, pubmedqa_data\n\n\ndef pubmed_train_test_split(datasetdict,train_size=0.75, \n                         strat_col='decision_encoded'):\n    #Convert dataset to pandas DataFrame\n    df = pd.DataFrame(datasetdict['train'])\n    test_size=(1-train_size)\n    # Define the stratification column\n    stratify_col=strat_col\n\n    #Split like normal\n    train_df, test_df = train_test_split(\n        df,\n        test_size=test_size,\n        stratify=df[stratify_col],\n        random_state=42)\n    # Convert DataFrames back to Dataset\n    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n    test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n\n    return train_dataset, test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:23:52.581183Z","iopub.execute_input":"2024-08-05T19:23:52.581581Z","iopub.status.idle":"2024-08-05T19:23:54.772693Z","shell.execute_reply.started":"2024-08-05T19:23:52.581538Z","shell.execute_reply":"2024-08-05T19:23:54.771845Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n# Initialize a defaultdict to hold the bucket counts\nlength_buckets = defaultdict(int)\n\n# Define the bucket size\nbucket_size = 128\n\n# Loop through each string in the list\nfor s in pubmedqa_train['long_answer']:\n    # Determine the bucket for the current string length\n    bucket = (len(s) // bucket_size) * bucket_size\n    # Increment the count for the appropriate bucket\n    length_buckets[bucket] += 1\n\n# Display the counts for each bucket\nfor bucket, count in sorted(length_buckets.items()):\n    print(f\"Length {bucket} - {bucket + bucket_size - 1}: {count} strings\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:23:56.385918Z","iopub.execute_input":"2024-08-05T19:23:56.386272Z","iopub.status.idle":"2024-08-05T19:23:56.395230Z","shell.execute_reply.started":"2024-08-05T19:23:56.386240Z","shell.execute_reply":"2024-08-05T19:23:56.394330Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Length 0 - 127: 66 strings\nLength 128 - 255: 326 strings\nLength 256 - 383: 244 strings\nLength 384 - 511: 82 strings\nLength 512 - 639: 25 strings\nLength 640 - 767: 4 strings\nLength 768 - 895: 3 strings\n","output_type":"stream"}]},{"cell_type":"code","source":"pubmedqa_artificial_df","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:57:04.681053Z","iopub.execute_input":"2024-08-05T19:57:04.681501Z","iopub.status.idle":"2024-08-05T19:57:04.712812Z","shell.execute_reply.started":"2024-08-05T19:57:04.681471Z","shell.execute_reply":"2024-08-05T19:57:04.711940Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                                 question  \\\n0       Are group 2 innate lymphoid cells ( ILC2s ) in...   \n1       Does vagus nerve contribute to the development...   \n2       Does psammaplin A induce Sirtuin 1-dependent a...   \n3       Is methylation of the FGFR2 gene associated wi...   \n4       Do tumor-infiltrating immune cell profiles and...   \n...                                                   ...   \n211264  Is urine production rate related to behavioura...   \n211265  Does evaluation of the use of general practice...   \n211266  Does intracoronary angiotensin-converting enzy...   \n211267  Does transfusion significantly increase the ri...   \n211268  Is low intramucosal pH associated with failure...   \n\n                                              long_answer     pubid  \\\n0       As ILC2s are elevated in patients with CRSwNP,...  25429730   \n1       Neuronal signals via the hepatic vagus nerve c...  25433161   \n2       PsA significantly inhibited MCF-7/adr cells pr...  25445714   \n3       We identified a novel biologically plausible c...  25431941   \n4       Breast cancer immune cell subpopulation profil...  25432519   \n...                                                   ...       ...   \n211264  During active sleep (state 2F) hourly fetal ur...   8217974   \n211265  General practice registers can provide a suita...   8204319   \n211266  Intracoronary enalaprilat resulted in an impro...   8205673   \n211267  The choice between splenectomy and splenic rep...   8215873   \n211268  Some critically ill patients with low gastric ...   8201088   \n\n       final_decision                                            context  \\\n0                 yes  {'contexts': ['Chronic rhinosinusitis (CRS) is...   \n1                 yes  {'contexts': ['Phosphatidylethanolamine N-meth...   \n2                 yes  {'contexts': ['Psammaplin A (PsA) is a natural...   \n3                 yes  {'contexts': ['This study examined links betwe...   \n4                 yes  {'contexts': ['Tumor microenvironment immunity...   \n...               ...                                                ...   \n211264            yes  {'contexts': ['To investigate the relation bet...   \n211265            yes  {'contexts': ['This study set out to show how ...   \n211266            yes  {'contexts': ['There is increasing recognition...   \n211267            yes  {'contexts': ['To determine if splenectomy res...   \n211268            yes  {'contexts': ['To determine if low gastric int...   \n\n        decision_encoded  \n0                      2  \n1                      2  \n2                      2  \n3                      2  \n4                      2  \n...                  ...  \n211264                 2  \n211265                 2  \n211266                 2  \n211267                 2  \n211268                 2  \n\n[211269 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>long_answer</th>\n      <th>pubid</th>\n      <th>final_decision</th>\n      <th>context</th>\n      <th>decision_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Are group 2 innate lymphoid cells ( ILC2s ) in...</td>\n      <td>As ILC2s are elevated in patients with CRSwNP,...</td>\n      <td>25429730</td>\n      <td>yes</td>\n      <td>{'contexts': ['Chronic rhinosinusitis (CRS) is...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Does vagus nerve contribute to the development...</td>\n      <td>Neuronal signals via the hepatic vagus nerve c...</td>\n      <td>25433161</td>\n      <td>yes</td>\n      <td>{'contexts': ['Phosphatidylethanolamine N-meth...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Does psammaplin A induce Sirtuin 1-dependent a...</td>\n      <td>PsA significantly inhibited MCF-7/adr cells pr...</td>\n      <td>25445714</td>\n      <td>yes</td>\n      <td>{'contexts': ['Psammaplin A (PsA) is a natural...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Is methylation of the FGFR2 gene associated wi...</td>\n      <td>We identified a novel biologically plausible c...</td>\n      <td>25431941</td>\n      <td>yes</td>\n      <td>{'contexts': ['This study examined links betwe...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Do tumor-infiltrating immune cell profiles and...</td>\n      <td>Breast cancer immune cell subpopulation profil...</td>\n      <td>25432519</td>\n      <td>yes</td>\n      <td>{'contexts': ['Tumor microenvironment immunity...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>211264</th>\n      <td>Is urine production rate related to behavioura...</td>\n      <td>During active sleep (state 2F) hourly fetal ur...</td>\n      <td>8217974</td>\n      <td>yes</td>\n      <td>{'contexts': ['To investigate the relation bet...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>211265</th>\n      <td>Does evaluation of the use of general practice...</td>\n      <td>General practice registers can provide a suita...</td>\n      <td>8204319</td>\n      <td>yes</td>\n      <td>{'contexts': ['This study set out to show how ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>211266</th>\n      <td>Does intracoronary angiotensin-converting enzy...</td>\n      <td>Intracoronary enalaprilat resulted in an impro...</td>\n      <td>8205673</td>\n      <td>yes</td>\n      <td>{'contexts': ['There is increasing recognition...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>211267</th>\n      <td>Does transfusion significantly increase the ri...</td>\n      <td>The choice between splenectomy and splenic rep...</td>\n      <td>8215873</td>\n      <td>yes</td>\n      <td>{'contexts': ['To determine if splenectomy res...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>211268</th>\n      <td>Is low intramucosal pH associated with failure...</td>\n      <td>Some critically ill patients with low gastric ...</td>\n      <td>8201088</td>\n      <td>yes</td>\n      <td>{'contexts': ['To determine if low gastric int...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>211269 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1.2.2. Importing artificial pubmedqa dataset","metadata":{}},{"cell_type":"code","source":"bioasq, pubmedqa_artificial = load_bioasq_pubmedqa(pubmed_kaggle_path='/kaggle/input/pubmed-qa/pubmed_qa_pga_artificial.parquet')","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:23:56.396276Z","iopub.execute_input":"2024-08-05T19:23:56.396540Z","iopub.status.idle":"2024-08-05T19:24:49.285616Z","shell.execute_reply.started":"2024-08-05T19:23:56.396517Z","shell.execute_reply":"2024-08-05T19:24:49.284654Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/211269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ba1a0bfa8d41a295f0e9e04ad658aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/211269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8750dd43ccc8497d8dc83b71d33fd384"}},"metadata":{}}]},{"cell_type":"code","source":"stratified_sample_df = pubmedqa_artificial_df.groupby('final_decision', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000 // len(pubmedqa_artificial_df['final_decision'].unique())), random_state=42))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T19:59:29.932052Z","iopub.execute_input":"2024-08-05T19:59:29.932465Z","iopub.status.idle":"2024-08-05T19:59:30.023936Z","shell.execute_reply.started":"2024-08-05T19:59:29.932436Z","shell.execute_reply":"2024-08-05T19:59:30.022914Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1041126199.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  stratified_sample_df = pubmedqa_artificial_df.groupby('final_decision', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000 // len(pubmedqa_artificial_df['final_decision'].unique())), random_state=42))\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict\n\n# Convert to pandas DataFrame to handle schema mismatch\npubmedqa_df = pubmedqa_train.to_pandas()\npubmedqa_artificial_df = pubmedqa_artificial['train'].to_pandas()\n# Ensure both DataFrames have the same columns\ncommon_columns = list(set(pubmedqa_df.columns).intersection(set(pubmedqa_artificial_df.columns)))\n\npubmedqa_df = pubmedqa_df[common_columns]\npubmedqa_artificial_df = pubmedqa_artificial_df[common_columns]\n\n# Take 1000 rows from artificial\npubmedqa_artificial_sample =  pubmedqa_artificial_df.groupby('final_decision', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000 // len(pubmedqa_artificial_df['final_decision'].unique())), random_state=42))\n\n# Step 3: Combine the samples to create pubmed_mix\ncombined_df = pd.concat([pubmedqa_df, pubmedqa_artificial_sample], ignore_index=True)\n\n# Step 4: Shuffle the combined DataFrame to mix the rows\npubmed_mix_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Step 5: Convert back to DatasetDict format\npubmed_mix = Dataset.from_pandas(pubmed_mix_df)\n\n# Create DatasetDict\n#pubmedqa_mix = DatasetDict({'train': pubmed_mix})\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:02:57.828694Z","iopub.execute_input":"2024-08-05T20:02:57.829554Z","iopub.status.idle":"2024-08-05T20:02:59.760334Z","shell.execute_reply.started":"2024-08-05T20:02:57.829520Z","shell.execute_reply":"2024-08-05T20:02:59.759355Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'long_answer', 'pubid', 'final_decision', 'context', 'decision_encoded'],\n        num_rows: 1750\n    })\n})\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2478928306.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  pubmedqa_artificial_sample =  pubmedqa_artificial_df.groupby('final_decision', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000 // len(pubmedqa_artificial_df['final_decision'].unique())), random_state=42))\n","output_type":"stream"}]},{"cell_type":"code","source":"responses = pubmed_mix['final_decision']\n# Counting the occurrences of each value\nyes_count = responses.count('yes')\nno_count = responses.count('no')\nmaybe_count = responses.count('maybe')\n\n# Display the counts\nprint(f\"Yes: {yes_count}\")\nprint(f\"No: {no_count}\")\nprint(f\"Maybe: {maybe_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:03:29.107588Z","iopub.execute_input":"2024-08-05T20:03:29.108412Z","iopub.status.idle":"2024-08-05T20:03:29.117490Z","shell.execute_reply.started":"2024-08-05T20:03:29.108380Z","shell.execute_reply":"2024-08-05T20:03:29.116409Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Yes: 914\nNo: 754\nMaybe: 82\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2Model\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\nimport gc\n\nclass QAModel(nn.Module):\n    def __init__(self, model, classes=3, dropout_prob=0.5):\n        super(QAModel, self).__init__()\n        self.bert = model\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.linear1 = nn.Linear(model.config.hidden_size, 128)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.linear2 = nn.Linear(128, classes)  # number of classes may vary between BioASQ (2 classes) and PubMedQA (3 classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n        cls_output = self.dropout1(cls_output)  # Apply first dropout\n        cls_output = self.linear1(cls_output)  # Apply first linear layer\n        cls_output = self.dropout2(cls_output)  # Apply second dropout\n        logits = self.linear2(cls_output)  # Apply second linear layer\n        return logits\n\nclass Trainandtest:\n\n    def __init__(self, df_train, df_test, stratify_col='decision_encoded'):\n        self.train_data = df_train\n        self.test_data = df_test\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.stratify_col = stratify_col\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.results={}\n\n    def initialize_tokenizer(self, model_name, source):\n        if isinstance(source, tuple):\n                source = source[0]\n        if 'GPT' in model_name:\n            tokenizer = GPT2Tokenizer.from_pretrained(source)\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n            return tokenizer\n        elif 'BioLinkBERT' in model_name or 'LinkBERT' in model_name:\n            return AutoTokenizer.from_pretrained(source)\n        else:\n            return BertTokenizer.from_pretrained(source)\n\n    def encode_data(self, df, tokenizer):\n        inputs = tokenizer(\n            text=df['question'], \n            text_pair=df['long_answer'], \n            padding=True, \n            truncation=True, \n            return_tensors='pt', \n            max_length=128*4\n        )\n        labels = torch.tensor(df[self.stratify_col])\n        return inputs, labels\n\n    def create_dataloader(self, inputs, labels, batch_size):\n        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    def import_model(self, QAModel, model_name, source, tokenizer):\n        if isinstance(source, tuple):\n            source = source[0]\n        if 'GPT' in model_name:\n            model = GPT2Model.from_pretrained(source)\n            model.resize_token_embeddings(len(tokenizer))\n            model = QAModel(model)\n        elif 'BioLinkBERT' in model_name or 'LinkBERT' in model_name:\n            model = AutoModel.from_pretrained(source)\n            model = QAModel(model)\n        else:\n            model = BertModel.from_pretrained(source)\n            model = QAModel(model)\n        return model\n    def model_compile(self, QAModel, model_name, source, batch_size=64, adamw=True):\n        batch_size = 16 if 'GPT' in model_name else batch_size\n        tokenizer = self.initialize_tokenizer(model_name, source)\n        train_inputs, train_labels = self.encode_data(self.train_data, tokenizer)\n        test_inputs, test_labels = self.encode_data(self.test_data, tokenizer)\n        self.train_loader = self.create_dataloader(train_inputs, train_labels, batch_size)\n        self.test_loader = self.create_dataloader(test_inputs, test_labels, batch_size)\n        \n        self.model = self.import_model(QAModel, model_name, source, tokenizer).to(self.device) \n        if adamw:\n            self.optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)\n        else: \n            self.optimizer = optim.Adam(self.model.parameters(), lr=2e-5)\n    \n    def training(self, model_name, epochs=10):\n        if isinstance(model_name, tuple):\n            model_name = model_name[0]        \n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            all_preds = []\n            all_labels = []\n        \n            for batch in self.train_loader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                self.optimizer.zero_grad()\n            \n                outputs = self.model(b_input_ids, b_attention_mask)\n                loss = self.loss_fn(outputs, b_labels)\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n                preds = outputs.detach().cpu().numpy()\n                label_ids = b_labels.to('cpu').numpy()\n                del b_input_ids \n                del b_attention_mask \n                del b_labels\n                gc.collect()\n                torch.cuda.empty_cache()\n                all_preds.append(preds)\n                all_labels.append(label_ids)\n        \n            avg_loss = total_loss / len(self.train_loader)\n            all_preds = np.concatenate(all_preds, axis=0)\n            all_labels = np.concatenate(all_labels, axis=0)\n            avg_f1_score = self.calculate_f1_score(all_preds, all_labels)\n        \n            print(f\"Epoch {epoch+1}, Loss: {avg_loss}, F1 Score: {avg_f1_score}\")\n        \n        self.save_model(model_name)\n\n    def save_model(self, model_name):\n        os.makedirs('/kaggle/working/models', exist_ok=True)\n        model_path = f'/kaggle/working/models/{model_name}_model.pth'\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, model_path)\n        print(f\"Model saved to {model_path}\")\n\n    def load_model(self, model_path):\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(f\"Model loaded from {model_path}\")\n\n    def calculate_f1_score(self, preds, labels):\n        preds_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return f1_score(labels_flat, preds_flat, average='weighted')\n\n    def evaluate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        predictions, true_labels = [], []\n    \n        with torch.no_grad():\n            for batch in dataloader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                outputs = self.model(b_input_ids, b_attention_mask)\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                predictions.extend(np.argmax(logits, axis=1))\n                true_labels.extend(label_ids)\n                del b_input_ids \n                del b_attention_mask \n                del b_labels\n                gc.collect()\n                torch.cuda.empty_cache()\n    \n        accuracy = accuracy_score(true_labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n    \n        return accuracy, precision, recall, f1\n        \n\n    def val(self, load_model_path=None):\n        if load_model_path:\n            self.load_model(load_model_path)\n                \n        test_accuracy, test_precision, test_recall, test_f1 = self.evaluate(self.test_loader)\n        print(f\"Test - Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1-Score: {test_f1}\")\n    \n        return {\n            'test': {\n                'accuracy': test_accuracy,\n                'precision': test_precision,\n                'recall': test_recall,\n                'f1': test_f1\n            }\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:40:17.873224Z","iopub.execute_input":"2024-08-06T00:40:17.873666Z","iopub.status.idle":"2024-08-06T00:40:17.912801Z","shell.execute_reply.started":"2024-08-06T00:40:17.873632Z","shell.execute_reply":"2024-08-06T00:40:17.911693Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def result_convert(result_dict):\n    df = pd.DataFrame({\n        'Model': result_dict.keys(),\n        'Accuracy': [result_dict[model]['test']['accuracy'] for model in result_dict],\n        'Precision': [result_dict[model]['test']['precision'] for model in result_dict],\n        'Recall': [result_dict[model]['test']['recall'] for model in result_dict],\n        'F1 Score': [result_dict[model]['test']['f1'] for model in result_dict]})\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:40:29.341399Z","iopub.execute_input":"2024-08-06T00:40:29.342233Z","iopub.status.idle":"2024-08-06T00:40:29.348104Z","shell.execute_reply.started":"2024-08-06T00:40:29.342200Z","shell.execute_reply":"2024-08-06T00:40:29.347050Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"models = [\n    \n    {\n        'model_name': 'BERT',\n        'source': 'bert-base-uncased',\n    },\n    {\n        'model_name': 'GPT',\n        'source': 'gpt2',\n    },\n    {\n        'model_name': 'ColBERT',\n        'source': 'colbert-ir/colbertv2.0',\n    },\n\n    {\n        'model_name': 'LinkBERT',\n        'source': 'michiyasunaga/LinkBERT-base',\n    },\n    {\n        'model_name': 'BiomedNLP',\n        'source': 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract',\n    },\n    {\n        'model_name': 'BioLinkBERT',\n        'source': 'michiyasunaga/BioLinkBERT-base',\n    },\n\n]","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:37:23.204786Z","iopub.execute_input":"2024-08-06T00:37:23.205453Z","iopub.status.idle":"2024-08-06T00:37:23.211118Z","shell.execute_reply.started":"2024-08-06T00:37:23.205422Z","shell.execute_reply":"2024-08-06T00:37:23.210203Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"trainer = Trainandtest(pubmedqa_train, pubmedqa_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer.model_compile(QAModel, model_name,source, batch_size=8)\n    # Train the model\n    trainer.training(model_name, epochs=3)\n    \n    # test the model\n    test_result = trainer.val()\n    trainer.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:04:05.684101Z","iopub.execute_input":"2024-08-05T21:04:05.684386Z","iopub.status.idle":"2024-08-05T21:19:02.063144Z","shell.execute_reply.started":"2024-08-05T21:04:05.684352Z","shell.execute_reply":"2024-08-05T21:19:02.062184Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9006132993926393, F1 Score: 0.5249099889435542\nEpoch 2, Loss: 0.7098345870667315, F1 Score: 0.6958092577262693\nEpoch 3, Loss: 0.5697074686276152, F1 Score: 0.7611704025147913\nModel saved to /kaggle/working/models/BERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.748, Precision: 0.6694068965517241, Recall: 0.748, F1-Score: 0.70531291715744\nEpoch 1, Loss: 1.393192358473514, F1 Score: 0.40785338135254096\nEpoch 2, Loss: 1.1123240907141503, F1 Score: 0.41203555542772197\nEpoch 3, Loss: 1.032146523607538, F1 Score: 0.43241287380838644\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9623821656754676, F1 Score: 0.4707225929057005\nEpoch 2, Loss: 0.7839130653028793, F1 Score: 0.6467877762783025\nEpoch 3, Loss: 0.5677921136325979, F1 Score: 0.7601542321805421\nModel saved to /kaggle/working/models/ColBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.736, Precision: 0.6570966841650607, Recall: 0.736, F1-Score: 0.6938642056690837\nEpoch 1, Loss: 0.9824871433542129, F1 Score: 0.45623053111762785\nEpoch 2, Loss: 0.8014798021696984, F1 Score: 0.6310760208825001\nEpoch 3, Loss: 0.6627186527277561, F1 Score: 0.7320635448666813\nModel saved to /kaggle/working/models/LinkBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.74, Precision: 0.6640471698113207, Recall: 0.74, F1-Score: 0.6985478163493841\nEpoch 1, Loss: 0.9246221917106751, F1 Score: 0.5620592798606044\nEpoch 2, Loss: 0.6177987788427384, F1 Score: 0.7432733337182548\nEpoch 3, Loss: 0.4287259772102884, F1 Score: 0.8118622911910612\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.756, Precision: 0.6822384779399705, Recall: 0.756, F1-Score: 0.7070682119205298\nEpoch 1, Loss: 0.9633053610933587, F1 Score: 0.4426174883428436\nEpoch 2, Loss: 0.7026493197425883, F1 Score: 0.7145666029027515\nEpoch 3, Loss: 0.550870918213053, F1 Score: 0.7752358324651903\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.784, Precision: 0.7036995305164319, Recall: 0.784, F1-Score: 0.7395857142857143\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing with adam instead of AdamW ","metadata":{}},{"cell_type":"code","source":"trainer_adam = Trainandtest(pubmedqa_train, pubmedqa_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_adam.model_compile(QAModel, model_name,source, adamw=False,batch_size=8)\n    # Train the model\n    trainer_adam.training(model_name, epochs=3)\n    \n    # test the model\n    test_result = trainer_adam.val()\n    trainer_adam.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:49:19.956115Z","iopub.execute_input":"2024-08-05T20:49:19.956818Z","iopub.status.idle":"2024-08-05T21:04:05.668735Z","shell.execute_reply.started":"2024-08-05T20:49:19.956789Z","shell.execute_reply":"2024-08-05T21:04:05.667637Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9944657968713883, F1 Score: 0.4253417483729644\nEpoch 2, Loss: 0.7733636637951465, F1 Score: 0.6548607825295722\nEpoch 3, Loss: 0.5842612776508991, F1 Score: 0.7550026857380494\nModel saved to /kaggle/working/models/BERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.756, Precision: 0.6714458404074704, Recall: 0.756, F1-Score: 0.7112167330829219\nEpoch 1, Loss: 1.2827900267661887, F1 Score: 0.43629321470457905\nEpoch 2, Loss: 1.064146750784935, F1 Score: 0.43621814171491585\nEpoch 3, Loss: 1.0238550039047891, F1 Score: 0.450740495819702\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9681339682416713, F1 Score: 0.49036236684242235\nEpoch 2, Loss: 0.8045094900942863, F1 Score: 0.6229481668773704\nEpoch 3, Loss: 0.5787445804540147, F1 Score: 0.7569829223879129\nModel saved to /kaggle/working/models/ColBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.724, Precision: 0.6443233082706766, Recall: 0.724, F1-Score: 0.6817145888594165\nEpoch 1, Loss: 0.9647965082462798, F1 Score: 0.4447860985053272\nEpoch 2, Loss: 0.7196468939172461, F1 Score: 0.6949428613612875\nEpoch 3, Loss: 0.5734080284675385, F1 Score: 0.760998794755239\nModel saved to /kaggle/working/models/LinkBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.788, Precision: 0.7010075187969925, Recall: 0.788, F1-Score: 0.7418228116710875\nEpoch 1, Loss: 0.9780284312177212, F1 Score: 0.5217758791110249\nEpoch 2, Loss: 0.6510280216944978, F1 Score: 0.7361572286721654\nEpoch 3, Loss: 0.4577103276202019, F1 Score: 0.8080678988997622\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.796, Precision: 0.7685895642818721, Recall: 0.796, F1-Score: 0.7588327432375017\nEpoch 1, Loss: 0.9089553800035031, F1 Score: 0.5260186850522093\nEpoch 2, Loss: 0.6385407179911086, F1 Score: 0.7309744816418116\nEpoch 3, Loss: 0.4850178955400244, F1 Score: 0.7964205240258372\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.824, Precision: 0.7824133333333333, Recall: 0.824, F1-Score: 0.7913226315503075\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# mixing artificial data ","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:41:12.449740Z","iopub.execute_input":"2024-08-06T00:41:12.450768Z","iopub.status.idle":"2024-08-06T00:41:12.456701Z","shell.execute_reply.started":"2024-08-06T00:41:12.450725Z","shell.execute_reply":"2024-08-06T00:41:12.455512Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"trainer_mix = Trainandtest(pubmed_mix, pubmedqa_test)\n\n# Function to free up memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_mix.model_compile(QAModel,model_name,source,batch_size=8)\n    free_memory()\n    # Train the model\n    trainer_mix.training(model_name, epochs=3)\n    \n    # test the model\n    test_result = trainer_mix.val()\n    trainer_mix.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:04:06.011310Z","iopub.execute_input":"2024-08-05T20:04:06.012075Z","iopub.status.idle":"2024-08-05T20:45:54.180413Z","shell.execute_reply.started":"2024-08-05T20:04:06.012045Z","shell.execute_reply":"2024-08-05T20:45:54.179352Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6736238362378182, F1 Score: 0.691544910967982\nEpoch 2, Loss: 0.465376404594613, F1 Score: 0.8295694071702636\nEpoch 3, Loss: 0.33468898533754154, F1 Score: 0.8769848317757546\nModel saved to /kaggle/working/models/BERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.744, Precision: 0.6626754966887417, Recall: 0.744, F1-Score: 0.7007689602359748\nEpoch 1, Loss: 1.1697420526634563, F1 Score: 0.4557046728980926\nEpoch 2, Loss: 0.9441103918985887, F1 Score: 0.4662551558983837\nEpoch 3, Loss: 0.9006876452402635, F1 Score: 0.4617393230350778\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.6935228329134858, F1 Score: 0.6728090010628573\nEpoch 2, Loss: 0.4662316624568478, F1 Score: 0.8209139196858206\nEpoch 3, Loss: 0.31377549711870006, F1 Score: 0.8815665162193244\nModel saved to /kaggle/working/models/ColBERT_model.pth\nTest - Accuracy: 0.732, Precision: 0.6584333333333334, Recall: 0.732, F1-Score: 0.6919905437352245\nEpoch 1, Loss: 0.6730421713838294, F1 Score: 0.6990942692374034\nEpoch 2, Loss: 0.46076451884965375, F1 Score: 0.8314749850609237\nEpoch 3, Loss: 0.3329331569169482, F1 Score: 0.8858639456298941\nModel saved to /kaggle/working/models/LinkBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.756, Precision: 0.6699550802139038, Recall: 0.756, F1-Score: 0.7092652176460249\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db50e9f8335c4bc38f04c17fad9b98e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7315bfa399f447a98333cc809ee94e45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b712330218a7445d9c3325e2fcd5160c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bce695ef0d64fc5af0f698f9dd3a006"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Loss: 0.6295522175150919, F1 Score: 0.7207359059710398\nEpoch 2, Loss: 0.393806324506255, F1 Score: 0.8586774804932878\nEpoch 3, Loss: 0.2491684401892636, F1 Score: 0.9110065858041375\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.78, Precision: 0.6954216867469879, Recall: 0.78, F1-Score: 0.7336621493854395\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0067af8ba07647d39d3805e5a1580149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d31a67f0832436e8aeb96283d722b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27770e769fd54111bebce64eb866d2c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86547aef7e8f429d9a3195e22e11d3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/559 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3885c4d5aef44293ad74b21598aeae65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a457779669a40cb97acc29fd694dda2"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Loss: 0.6713117689030356, F1 Score: 0.695257681121867\nEpoch 2, Loss: 0.42754317891516097, F1 Score: 0.8409749716283901\nEpoch 3, Loss: 0.3084790704763371, F1 Score: 0.8889605442515863\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.804, Precision: 0.7171698595146871, Recall: 0.804, F1-Score: 0.7574175438596492\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing with bidirectional LSTM for output","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass BiLSTMmodel(nn.Module):\n    def __init__(self, model, classes=3, lstm_hidden_size=256, lstm_layers=1, dropout_prob=0.5):\n        super(BiLSTMmodel, self).__init__()\n        self.bert = model\n        self.dropout1 = nn.Dropout(dropout_prob)\n        \n        # BiLSTM layer\n        self.lstm = nn.LSTM(input_size=model.config.hidden_size,\n                            hidden_size=lstm_hidden_size,\n                            num_layers=lstm_layers,\n                            batch_first=True,\n                            bidirectional=True)\n        \n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.linear1 = nn.Linear(lstm_hidden_size * 2, 128)  # Bidirectional LSTM hidden size is doubled\n        self.dropout3 = nn.Dropout(dropout_prob)\n        self.linear2 = nn.Linear(128, classes)  # number of classes may vary between BioASQ (2 classes) and PubMedQA (3 classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state  # BERT outputs\n        \n        # Pass BERT outputs through BiLSTM\n        lstm_output, _ = self.lstm(sequence_output)\n        lstm_output = lstm_output[:, 0, :]  # Use the hidden state of the first token (CLS token)\n        \n        lstm_output = self.dropout2(lstm_output)\n        lstm_output = self.linear1(lstm_output)\n        lstm_output = self.dropout3(lstm_output)\n        logits = self.linear2(lstm_output)\n        \n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:27:41.966132Z","iopub.execute_input":"2024-08-05T21:27:41.966487Z","iopub.status.idle":"2024-08-05T21:27:41.976183Z","shell.execute_reply.started":"2024-08-05T21:27:41.966458Z","shell.execute_reply":"2024-08-05T21:27:41.975193Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"trainer_bilstm = Trainandtest(pubmedqa_train, pubmedqa_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_bilstm.model_compile(BiLSTMmodel, model_name,source,batch_size=8)\n    # Train the model\n    trainer_bilstm.training(model_name, epochs=3)\n    \n    # test the model\n    test_result = trainer_bilstm.val()\n    trainer_bilstm.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:30:18.605518Z","iopub.execute_input":"2024-08-05T21:30:18.605897Z","iopub.status.idle":"2024-08-05T21:45:07.039839Z","shell.execute_reply.started":"2024-08-05T21:30:18.605858Z","shell.execute_reply":"2024-08-05T21:45:07.038943Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9635301663520488, F1 Score: 0.48290765593552526\nEpoch 2, Loss: 0.7392633834734876, F1 Score: 0.6988844637095462\nEpoch 3, Loss: 0.5267599085544018, F1 Score: 0.7794557473839665\nModel saved to /kaggle/working/models/BERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.74, Precision: 0.6578823529411766, Recall: 0.74, F1-Score: 0.6939056065885334\nEpoch 1, Loss: 1.3115251089664215, F1 Score: 0.43367769678223544\nEpoch 2, Loss: 1.0169418936080121, F1 Score: 0.4643370417796646\nEpoch 3, Loss: 1.0267750785705891, F1 Score: 0.4273241014715746\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9932269111592719, F1 Score: 0.42351438326931284\nEpoch 2, Loss: 0.8081247873128728, F1 Score: 0.642997106285956\nEpoch 3, Loss: 0.6008845420276865, F1 Score: 0.7443287563364338\nModel saved to /kaggle/working/models/ColBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.728, Precision: 0.6462502902252147, Recall: 0.728, F1-Score: 0.6789677888989991\nEpoch 1, Loss: 0.9748588819453057, F1 Score: 0.44215684125563953\nEpoch 2, Loss: 0.8172687174791985, F1 Score: 0.6304076789339947\nEpoch 3, Loss: 0.6159245508148316, F1 Score: 0.7505618500273673\nModel saved to /kaggle/working/models/LinkBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.74, Precision: 0.6620484720758694, Recall: 0.74, F1-Score: 0.6978807311956847\nEpoch 1, Loss: 0.9573984653391736, F1 Score: 0.5095489964580875\nEpoch 2, Loss: 0.6249953443382649, F1 Score: 0.7370382056805996\nEpoch 3, Loss: 0.4561533945751317, F1 Score: 0.8216667785353308\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.804, Precision: 0.7839732136878056, Recall: 0.804, F1-Score: 0.7713454922279793\nEpoch 1, Loss: 0.9686440977644413, F1 Score: 0.424868906455863\nEpoch 2, Loss: 0.768838274986186, F1 Score: 0.6491089457125033\nEpoch 3, Loss: 0.5697488480425895, F1 Score: 0.7712638846583408\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.792, Precision: 0.7095912685445396, Recall: 0.792, F1-Score: 0.7467093961357157\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# bilstm + adam","metadata":{}},{"cell_type":"code","source":"trainer_bilstm_adam = Trainandtest(pubmedqa_train, pubmedqa_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_bilstm_adam.model_compile(BiLSTMmodel, model_name,source,adamw=False, batch_size=8)\n    # Train the model\n    trainer_bilstm_adam.training(model_name, epochs=3)\n    \n    # test the model\n    test_result = trainer_bilstm_adam.val()\n    trainer_bilstm_adam.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:24:42.048244Z","iopub.execute_input":"2024-08-05T22:24:42.048863Z","iopub.status.idle":"2024-08-05T22:39:01.283884Z","shell.execute_reply.started":"2024-08-05T22:24:42.048832Z","shell.execute_reply":"2024-08-05T22:39:01.282929Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9335505280722963, F1 Score: 0.4956809169038876\nEpoch 2, Loss: 0.7176162714653826, F1 Score: 0.7000559695528399\nEpoch 3, Loss: 0.5201842948952888, F1 Score: 0.7708864527705387\nModel saved to /kaggle/working/models/BERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.748, Precision: 0.6643477079796265, Recall: 0.748, F1-Score: 0.7036946250500505\nEpoch 1, Loss: 1.9795183552072404, F1 Score: 0.3883792578840153\nEpoch 2, Loss: 1.1531561511628172, F1 Score: 0.42933204453510676\nEpoch 3, Loss: 1.089423419313228, F1 Score: 0.42742713340781985\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9634516144052465, F1 Score: 0.4520106452132586\nEpoch 2, Loss: 0.7716564190514544, F1 Score: 0.6572602721961783\nEpoch 3, Loss: 0.5557666971011365, F1 Score: 0.7670569836602411\nModel saved to /kaggle/working/models/ColBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.72, Precision: 0.6373140477755862, Recall: 0.72, F1-Score: 0.6739880367189813\nEpoch 1, Loss: 0.95411590439208, F1 Score: 0.4542520625889047\nEpoch 2, Loss: 0.7634850464602734, F1 Score: 0.6668476381953001\nEpoch 3, Loss: 0.6080983522090506, F1 Score: 0.7512189133798105\nModel saved to /kaggle/working/models/LinkBERT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.764, Precision: 0.6777523390203632, Recall: 0.764, F1-Score: 0.7182162162162162\nEpoch 1, Loss: 0.9656948662818746, F1 Score: 0.5176955226765354\nEpoch 2, Loss: 0.6322591750228659, F1 Score: 0.7463169170551024\nEpoch 3, Loss: 0.4217598030858852, F1 Score: 0.82901014775011\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.776, Precision: 0.7099284082254379, Recall: 0.776, F1-Score: 0.7405554679234394\nEpoch 1, Loss: 0.9815204828343493, F1 Score: 0.45244928014067476\nEpoch 2, Loss: 0.7912213304575454, F1 Score: 0.6509258327824234\nEpoch 3, Loss: 0.5800189352098931, F1 Score: 0.7583522811161967\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.804, Precision: 0.7348923076923076, Recall: 0.804, F1-Score: 0.7603266022827041\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"result_adamw= result_convert(trainer.results)\nresult_adamw","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:19:02.066168Z","iopub.execute_input":"2024-08-05T21:19:02.066771Z","iopub.status.idle":"2024-08-05T21:19:02.079425Z","shell.execute_reply.started":"2024-08-05T21:19:02.066736Z","shell.execute_reply":"2024-08-05T21:19:02.078495Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.748   0.669407   0.748  0.705313\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.736   0.657097   0.736  0.693864\n3     LinkBERT     0.740   0.664047   0.740  0.698548\n4    BiomedNLP     0.756   0.682238   0.756  0.707068\n5  BioLinkBERT     0.784   0.703700   0.784  0.739586","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.748</td>\n      <td>0.669407</td>\n      <td>0.748</td>\n      <td>0.705313</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.736</td>\n      <td>0.657097</td>\n      <td>0.736</td>\n      <td>0.693864</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.740</td>\n      <td>0.664047</td>\n      <td>0.740</td>\n      <td>0.698548</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.756</td>\n      <td>0.682238</td>\n      <td>0.756</td>\n      <td>0.707068</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.784</td>\n      <td>0.703700</td>\n      <td>0.784</td>\n      <td>0.739586</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_adam= result_convert(trainer_adam.results)\nresult_adam","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:04:05.670433Z","iopub.execute_input":"2024-08-05T21:04:05.670734Z","iopub.status.idle":"2024-08-05T21:04:05.683065Z","shell.execute_reply.started":"2024-08-05T21:04:05.670708Z","shell.execute_reply":"2024-08-05T21:04:05.682180Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.756   0.671446   0.756  0.711217\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.724   0.644323   0.724  0.681715\n3     LinkBERT     0.788   0.701008   0.788  0.741823\n4    BiomedNLP     0.796   0.768590   0.796  0.758833\n5  BioLinkBERT     0.824   0.782413   0.824  0.791323","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.756</td>\n      <td>0.671446</td>\n      <td>0.756</td>\n      <td>0.711217</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.724</td>\n      <td>0.644323</td>\n      <td>0.724</td>\n      <td>0.681715</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.788</td>\n      <td>0.701008</td>\n      <td>0.788</td>\n      <td>0.741823</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.796</td>\n      <td>0.768590</td>\n      <td>0.796</td>\n      <td>0.758833</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.824</td>\n      <td>0.782413</td>\n      <td>0.824</td>\n      <td>0.791323</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_mix= result_convert(trainer_mix.results)\nresult_mix","metadata":{"execution":{"iopub.status.busy":"2024-08-05T20:48:10.741912Z","iopub.execute_input":"2024-08-05T20:48:10.742579Z","iopub.status.idle":"2024-08-05T20:48:10.756018Z","shell.execute_reply.started":"2024-08-05T20:48:10.742540Z","shell.execute_reply":"2024-08-05T20:48:10.755119Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.744   0.662675   0.744  0.700769\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.732   0.658433   0.732  0.691991\n3     LinkBERT     0.756   0.669955   0.756  0.709265\n4    BiomedNLP     0.780   0.695422   0.780  0.733662\n5  BioLinkBERT     0.804   0.717170   0.804  0.757418","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.744</td>\n      <td>0.662675</td>\n      <td>0.744</td>\n      <td>0.700769</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.732</td>\n      <td>0.658433</td>\n      <td>0.732</td>\n      <td>0.691991</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.756</td>\n      <td>0.669955</td>\n      <td>0.756</td>\n      <td>0.709265</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.780</td>\n      <td>0.695422</td>\n      <td>0.780</td>\n      <td>0.733662</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.804</td>\n      <td>0.717170</td>\n      <td>0.804</td>\n      <td>0.757418</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_bilstm= result_convert(trainer_bilstm.results)\nresult_bilstm","metadata":{"execution":{"iopub.status.busy":"2024-08-05T21:45:07.041701Z","iopub.execute_input":"2024-08-05T21:45:07.042197Z","iopub.status.idle":"2024-08-05T21:45:07.054454Z","shell.execute_reply.started":"2024-08-05T21:45:07.042158Z","shell.execute_reply":"2024-08-05T21:45:07.053608Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.740   0.657882   0.740  0.693906\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.728   0.646250   0.728  0.678968\n3     LinkBERT     0.740   0.662048   0.740  0.697881\n4    BiomedNLP     0.804   0.783973   0.804  0.771345\n5  BioLinkBERT     0.792   0.709591   0.792  0.746709","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.740</td>\n      <td>0.657882</td>\n      <td>0.740</td>\n      <td>0.693906</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.728</td>\n      <td>0.646250</td>\n      <td>0.728</td>\n      <td>0.678968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.740</td>\n      <td>0.662048</td>\n      <td>0.740</td>\n      <td>0.697881</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.804</td>\n      <td>0.783973</td>\n      <td>0.804</td>\n      <td>0.771345</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.792</td>\n      <td>0.709591</td>\n      <td>0.792</td>\n      <td>0.746709</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"result_bilstm_adam= result_convert(trainer_bilstm_adam.results)\nresult_bilstm_adam","metadata":{"execution":{"iopub.status.busy":"2024-08-05T22:39:01.285626Z","iopub.execute_input":"2024-08-05T22:39:01.285956Z","iopub.status.idle":"2024-08-05T22:39:01.298213Z","shell.execute_reply.started":"2024-08-05T22:39:01.285926Z","shell.execute_reply":"2024-08-05T22:39:01.297324Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.748   0.664348   0.748  0.703695\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.720   0.637314   0.720  0.673988\n3     LinkBERT     0.764   0.677752   0.764  0.718216\n4    BiomedNLP     0.776   0.709928   0.776  0.740555\n5  BioLinkBERT     0.804   0.734892   0.804  0.760327","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.748</td>\n      <td>0.664348</td>\n      <td>0.748</td>\n      <td>0.703695</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.720</td>\n      <td>0.637314</td>\n      <td>0.720</td>\n      <td>0.673988</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.764</td>\n      <td>0.677752</td>\n      <td>0.764</td>\n      <td>0.718216</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.776</td>\n      <td>0.709928</td>\n      <td>0.776</td>\n      <td>0.740555</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.804</td>\n      <td>0.734892</td>\n      <td>0.804</td>\n      <td>0.760327</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing with just context ","metadata":{}},{"cell_type":"code","source":"pubmed_text = pd.DataFrame(pubmedqa['train']['context'])\npubmed_text['full_context'] = pubmed_text['contexts'].apply(lambda x: ' '.join(x))\n# Convert to a DataFrame\npubmedqa_train_df = pd.DataFrame(pubmedqa['train'])\npubmedqa_train_df['full_context']= pubmed_text['full_context']\n\n# Convert the DataFrame back to a Dataset\npubmedqa_context = Dataset.from_pandas(pubmedqa_train_df)\n\n# Create a DatasetDict\npubmedqa = DatasetDict({\n    'train': pubmedqa_context\n})","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:31:09.817285Z","iopub.execute_input":"2024-08-06T00:31:09.818053Z","iopub.status.idle":"2024-08-06T00:31:10.122992Z","shell.execute_reply.started":"2024-08-06T00:31:09.818018Z","shell.execute_reply":"2024-08-06T00:31:10.122159Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pubmedqa_context_train, pubmedqa_context_test = pubmed_train_test_split(pubmedqa)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:34:31.137116Z","iopub.execute_input":"2024-08-06T00:34:31.137947Z","iopub.status.idle":"2024-08-06T00:34:31.390876Z","shell.execute_reply.started":"2024-08-06T00:34:31.137905Z","shell.execute_reply":"2024-08-06T00:34:31.390015Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2Model\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n\nclass QAModel(nn.Module):\n    def __init__(self, model, classes=3, dropout_prob=0.5):\n        super(QAModel, self).__init__()\n        self.bert = model\n        self.dropout1 = nn.Dropout(dropout_prob)\n        self.linear1 = nn.Linear(model.config.hidden_size, 128)\n        self.dropout2 = nn.Dropout(dropout_prob)\n        self.linear2 = nn.Linear(128, classes)  # number of classes may vary between BioASQ (2 classes) and PubMedQA (3 classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n        cls_output = self.dropout1(cls_output)  # Apply first dropout\n        cls_output = self.linear1(cls_output)  # Apply first linear layer\n        cls_output = self.dropout2(cls_output)  # Apply second dropout\n        logits = self.linear2(cls_output)  # Apply second linear layer\n        return logits\n\nclass Trainandtest:\n\n    def __init__(self, df_train, df_test, stratify_col='decision_encoded'):\n        self.train_data = df_train\n        self.test_data = df_test\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.stratify_col = stratify_col\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.results={}\n\n    def initialize_tokenizer(self, model_name, source):\n        if isinstance(source, tuple):\n                source = source[0]\n        if 'GPT' in model_name:\n            tokenizer = GPT2Tokenizer.from_pretrained(source)\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n            return tokenizer\n        elif 'BioLinkBERT' in model_name or 'LinkBERT' in model_name:\n            return AutoTokenizer.from_pretrained(source)\n        else:\n            return BertTokenizer.from_pretrained(source)\n\n    def encode_data(self, df, tokenizer):\n        inputs = tokenizer(\n            text=df['question'], \n            text_pair=df['full_context'], \n            padding=True, \n            truncation=True, \n            return_tensors='pt', \n            max_length=128*4\n        )\n        labels = torch.tensor(df[self.stratify_col])\n        return inputs, labels\n\n    def create_dataloader(self, inputs, labels, batch_size):\n        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    def import_model(self, QAModel, model_name, source, tokenizer):\n        if isinstance(source, tuple):\n            source = source[0]\n        if 'GPT' in model_name:\n            model = GPT2Model.from_pretrained(source)\n            model.resize_token_embeddings(len(tokenizer))\n            model = QAModel(model)\n        elif 'BioLinkBERT' in model_name or 'LinkBERT' in model_name:\n            model = AutoModel.from_pretrained(source)\n            model = QAModel(model)\n        else:\n            model = BertModel.from_pretrained(source)\n            model = QAModel(model)\n        return model\n    def model_compile(self, QAModel, model_name, source, batch_size=64, adamw=True):\n        batch_size = 16 if 'GPT' in model_name else batch_size\n        tokenizer = self.initialize_tokenizer(model_name, source)\n        train_inputs, train_labels = self.encode_data(self.train_data, tokenizer)\n        test_inputs, test_labels = self.encode_data(self.test_data, tokenizer)\n        self.train_loader = self.create_dataloader(train_inputs, train_labels, batch_size)\n        self.test_loader = self.create_dataloader(test_inputs, test_labels, batch_size)\n        \n        self.model = self.import_model(QAModel, model_name, source, tokenizer).to(self.device) \n        if adamw:\n            self.optimizer = optim.AdamW(self.model.parameters(), lr=2e-5)\n        else: \n            self.optimizer = optim.Adam(self.model.parameters(), lr=2e-5)\n    \n    def training(self, model_name, epochs=10):\n        if isinstance(model_name, tuple):\n            model_name = model_name[0]        \n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            all_preds = []\n            all_labels = []\n        \n            for batch in self.train_loader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                self.optimizer.zero_grad()\n            \n                outputs = self.model(b_input_ids, b_attention_mask)\n                loss = self.loss_fn(outputs, b_labels)\n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n                preds = outputs.detach().cpu().numpy()\n                label_ids = b_labels.to('cpu').numpy()\n                del b_input_ids \n                del b_attention_mask \n                del b_labels\n                gc.collect()\n                torch.cuda.empty_cache()\n                all_preds.append(preds)\n                all_labels.append(label_ids)\n        \n            avg_loss = total_loss / len(self.train_loader)\n            all_preds = np.concatenate(all_preds, axis=0)\n            all_labels = np.concatenate(all_labels, axis=0)\n            avg_f1_score = self.calculate_f1_score(all_preds, all_labels)\n        \n            print(f\"Epoch {epoch+1}, Loss: {avg_loss}, F1 Score: {avg_f1_score}\")\n        \n        self.save_model(model_name)\n\n    def save_model(self, model_name):\n        os.makedirs('/kaggle/working/models', exist_ok=True)\n        model_path = f'/kaggle/working/models/{model_name}_model.pth'\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, model_path)\n        print(f\"Model saved to {model_path}\")\n\n    def load_model(self, model_path):\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        print(f\"Model loaded from {model_path}\")\n\n    def calculate_f1_score(self, preds, labels):\n        preds_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return f1_score(labels_flat, preds_flat, average='weighted')\n\n    def evaluate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        predictions, true_labels = [], []\n    \n        with torch.no_grad():\n            for batch in dataloader:\n                b_input_ids, b_attention_mask, b_labels = [t.to(self.device) for t in batch]\n                outputs = self.model(b_input_ids, b_attention_mask)\n                logits = outputs.detach().cpu().numpy()\n                label_ids = b_labels.cpu().numpy()\n                predictions.extend(np.argmax(logits, axis=1))\n                true_labels.extend(label_ids)\n                del b_input_ids \n                del b_attention_mask \n                del b_labels\n                gc.collect()\n                torch.cuda.empty_cache()\n    \n        accuracy = accuracy_score(true_labels, predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n    \n        return accuracy, precision, recall, f1\n        \n\n    def val(self, load_model_path=None):\n        if load_model_path:\n            self.load_model(load_model_path)\n                \n        test_accuracy, test_precision, test_recall, test_f1 = self.evaluate(self.test_loader)\n        print(f\"Test - Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1-Score: {test_f1}\")\n    \n        return {\n            'test': {\n                'accuracy': test_accuracy,\n                'precision': test_precision,\n                'recall': test_recall,\n                'f1': test_f1\n            }\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-06T00:38:12.969932Z","iopub.execute_input":"2024-08-06T00:38:12.970453Z","iopub.status.idle":"2024-08-06T00:38:13.019805Z","shell.execute_reply.started":"2024-08-06T00:38:12.970409Z","shell.execute_reply":"2024-08-06T00:38:13.018785Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"trainer_context = Trainandtest(pubmedqa_context_train, pubmedqa_context_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_context.model_compile(QAModel, model_name,source, adamw=False,batch_size=32)\n    # Train the model\n    trainer_context.training(model_name, epochs=10)\n    \n    # test the model\n    test_result = trainer_context.val()\n    trainer_context.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:01:27.316828Z","iopub.execute_input":"2024-08-06T01:01:27.317190Z","iopub.status.idle":"2024-08-06T01:30:35.932487Z","shell.execute_reply.started":"2024-08-06T01:01:27.317162Z","shell.execute_reply":"2024-08-06T01:30:35.931567Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9956087817748388, F1 Score: 0.44379389794495544\nEpoch 2, Loss: 0.9260348826646805, F1 Score: 0.467974720792834\nEpoch 3, Loss: 0.7666239080329736, F1 Score: 0.6489225499984872\nEpoch 4, Loss: 0.6035305373370647, F1 Score: 0.7415143995953295\nEpoch 5, Loss: 0.47606802855928737, F1 Score: 0.7946067327830272\nEpoch 6, Loss: 0.34642056996623677, F1 Score: 0.8511775348168191\nEpoch 7, Loss: 0.2580205186580618, F1 Score: 0.8866075525251556\nEpoch 8, Loss: 0.18729669569681087, F1 Score: 0.9369975794868157\nEpoch 9, Loss: 0.13886863629644117, F1 Score: 0.960059241154904\nEpoch 10, Loss: 0.0845037210577478, F1 Score: 0.9779172647527911\nModel saved to /kaggle/working/models/BERT_model.pth\nTest - Accuracy: 0.684, Precision: 0.7294617424242424, Recall: 0.684, F1-Score: 0.7045881409897586\nEpoch 1, Loss: 1.3585337527254795, F1 Score: 0.44299407780965916\nEpoch 2, Loss: 1.0587855374559443, F1 Score: 0.45385088741681573\nEpoch 3, Loss: 1.0438256200323714, F1 Score: 0.44054302488042835\nEpoch 4, Loss: 0.9878683775029284, F1 Score: 0.49354679591466855\nEpoch 5, Loss: 0.9808363635489281, F1 Score: 0.4422310626114203\nEpoch 6, Loss: 1.006369097435728, F1 Score: 0.4510266666666666\nEpoch 7, Loss: 0.9873607082569853, F1 Score: 0.42259364620414847\nEpoch 8, Loss: 0.9720805832680236, F1 Score: 0.4336694367497692\nEpoch 9, Loss: 0.9838061675112298, F1 Score: 0.4328739013900143\nEpoch 10, Loss: 0.9873914756673448, F1 Score: 0.44477175624966975\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9520245417952538, F1 Score: 0.45297674916961317\nEpoch 2, Loss: 0.9212048103411993, F1 Score: 0.4826229757472229\nEpoch 3, Loss: 0.7950820500651995, F1 Score: 0.6363152721510011\nEpoch 4, Loss: 0.6057927769919237, F1 Score: 0.7363657106752968\nEpoch 5, Loss: 0.4558780913551648, F1 Score: 0.7984399635452267\nEpoch 6, Loss: 0.3412573526923855, F1 Score: 0.8496917808219178\nEpoch 7, Loss: 0.23623682030787072, F1 Score: 0.8973209773134148\nEpoch 8, Loss: 0.19447369469950596, F1 Score: 0.9264559794355036\nEpoch 9, Loss: 0.14640878606587648, F1 Score: 0.9546388083429811\nEpoch 10, Loss: 0.11481602140702307, F1 Score: 0.9595680799282695\nModel saved to /kaggle/working/models/ColBERT_model.pth\nTest - Accuracy: 0.688, Precision: 0.7030860215053764, Recall: 0.688, F1-Score: 0.6932197595858558\nEpoch 1, Loss: 1.0067957465847333, F1 Score: 0.4560220394560017\nEpoch 2, Loss: 0.9534691696365675, F1 Score: 0.47018975769482646\nEpoch 3, Loss: 0.8506327594319979, F1 Score: 0.5770253044837769\nEpoch 4, Loss: 0.6719858931998411, F1 Score: 0.7221594578098595\nEpoch 5, Loss: 0.5750337379674116, F1 Score: 0.7646362918415817\nEpoch 6, Loss: 0.4888757976392905, F1 Score: 0.7995690158689307\nEpoch 7, Loss: 0.37337694441278774, F1 Score: 0.8285398195351054\nEpoch 8, Loss: 0.3076048133273919, F1 Score: 0.8759715515732476\nEpoch 9, Loss: 0.255288806433479, F1 Score: 0.9122517629582557\nEpoch 10, Loss: 0.2350365286692977, F1 Score: 0.9182583058832222\nModel saved to /kaggle/working/models/LinkBERT_model.pth\nTest - Accuracy: 0.74, Precision: 0.7220040567951319, Recall: 0.74, F1-Score: 0.7300404234707034\nEpoch 1, Loss: 1.043401983877023, F1 Score: 0.5009785057447723\nEpoch 2, Loss: 0.7538712931176027, F1 Score: 0.671776418494283\nEpoch 3, Loss: 0.5878038182854652, F1 Score: 0.7531421084860656\nEpoch 4, Loss: 0.47556211178501445, F1 Score: 0.7979201401331778\nEpoch 5, Loss: 0.3583130855113268, F1 Score: 0.8511798485363749\nEpoch 6, Loss: 0.282158166791002, F1 Score: 0.8853246836866753\nEpoch 7, Loss: 0.25083431011686724, F1 Score: 0.8997010605292868\nEpoch 8, Loss: 0.1864325199276209, F1 Score: 0.9436548325405842\nEpoch 9, Loss: 0.1387742661560575, F1 Score: 0.9549643605870021\nEpoch 10, Loss: 0.13197371666319668, F1 Score: 0.9575081209473515\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.756, Precision: 0.7980760068065798, Recall: 0.756, F1-Score: 0.7741414926339646\nEpoch 1, Loss: 0.9820928474267324, F1 Score: 0.45296029069888116\nEpoch 2, Loss: 0.9307247524460157, F1 Score: 0.4748115867871966\nEpoch 3, Loss: 0.7906167283654213, F1 Score: 0.6553624457975108\nEpoch 4, Loss: 0.591247291614612, F1 Score: 0.7574629266644136\nEpoch 5, Loss: 0.4755264539271593, F1 Score: 0.8130565382591767\nEpoch 6, Loss: 0.3836271694550912, F1 Score: 0.8485965554995091\nEpoch 7, Loss: 0.33806396027406055, F1 Score: 0.8839604248163863\nEpoch 8, Loss: 0.30179049354046583, F1 Score: 0.8895104023136665\nEpoch 9, Loss: 0.24901557496438423, F1 Score: 0.9262358550814692\nEpoch 10, Loss: 0.2255850766475002, F1 Score: 0.9148985091612024\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.708, Precision: 0.8056955145595948, Recall: 0.708, F1-Score: 0.7385571964956196\n","output_type":"stream"}]},{"cell_type":"code","source":"result_context= result_convert(trainer_context.results)\nresult_context","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:33:17.997061Z","iopub.execute_input":"2024-08-06T01:33:17.997792Z","iopub.status.idle":"2024-08-06T01:33:18.010868Z","shell.execute_reply.started":"2024-08-06T01:33:17.997760Z","shell.execute_reply":"2024-08-06T01:33:18.009875Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.684   0.729462   0.684  0.704588\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.688   0.703086   0.688  0.693220\n3     LinkBERT     0.740   0.722004   0.740  0.730040\n4    BiomedNLP     0.756   0.798076   0.756  0.774141\n5  BioLinkBERT     0.708   0.805696   0.708  0.738557","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.684</td>\n      <td>0.729462</td>\n      <td>0.684</td>\n      <td>0.704588</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.688</td>\n      <td>0.703086</td>\n      <td>0.688</td>\n      <td>0.693220</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.740</td>\n      <td>0.722004</td>\n      <td>0.740</td>\n      <td>0.730040</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.756</td>\n      <td>0.798076</td>\n      <td>0.756</td>\n      <td>0.774141</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.708</td>\n      <td>0.805696</td>\n      <td>0.708</td>\n      <td>0.738557</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"trainer_context_adamw = Trainandtest(pubmedqa_context_train, pubmedqa_context_test)\n\nfor model in models:\n    model_name=model['model_name'],\n    source=model['source'],\n    trainer_context_adamw.model_compile(QAModel, model_name,source,batch_size=32)\n    # Train the model\n    trainer_context_adamw.training(model_name, epochs=10)\n    \n    # test the model\n    test_result = trainer_context_adamw.val()\n    trainer_context_adamw.results[model['model_name']] = test_result","metadata":{"execution":{"iopub.status.busy":"2024-08-06T01:34:59.654432Z","iopub.execute_input":"2024-08-06T01:34:59.655066Z","iopub.status.idle":"2024-08-06T02:03:22.278330Z","shell.execute_reply.started":"2024-08-06T01:34:59.655038Z","shell.execute_reply":"2024-08-06T02:03:22.277413Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9762519747018814, F1 Score: 0.4699483311583402\nEpoch 2, Loss: 0.923114595313867, F1 Score: 0.461531669637552\nEpoch 3, Loss: 0.7446715695162615, F1 Score: 0.6811459012673045\nEpoch 4, Loss: 0.5992327022055784, F1 Score: 0.7511168646563102\nEpoch 5, Loss: 0.47549499819676083, F1 Score: 0.787570027877593\nEpoch 6, Loss: 0.37996089334289235, F1 Score: 0.8323402266290735\nEpoch 7, Loss: 0.3250357086459796, F1 Score: 0.8703587289410076\nEpoch 8, Loss: 0.22723182663321495, F1 Score: 0.9252874017493735\nEpoch 9, Loss: 0.19044163528208932, F1 Score: 0.9426072087826678\nEpoch 10, Loss: 0.13953980958710113, F1 Score: 0.9535687984294463\nModel saved to /kaggle/working/models/BERT_model.pth\nTest - Accuracy: 0.728, Precision: 0.7235849731663685, Recall: 0.728, F1-Score: 0.7257289760348584\nEpoch 1, Loss: 1.4827121978110456, F1 Score: 0.4296229401969969\nEpoch 2, Loss: 1.110146200403254, F1 Score: 0.45954562368681473\nEpoch 3, Loss: 1.0427006292850414, F1 Score: 0.45145540092523667\nEpoch 4, Loss: 1.0313175358670823, F1 Score: 0.4659141544764547\nEpoch 5, Loss: 1.0125592642642083, F1 Score: 0.43187578296301066\nEpoch 6, Loss: 0.972374254084648, F1 Score: 0.4570682956676998\nEpoch 7, Loss: 1.009635405337557, F1 Score: 0.44708075173554146\nEpoch 8, Loss: 0.979262464858116, F1 Score: 0.44383639256366525\nEpoch 9, Loss: 0.9730009905835415, F1 Score: 0.44147458418651436\nEpoch 10, Loss: 0.9721710821415516, F1 Score: 0.45375272937506717\nModel saved to /kaggle/working/models/GPT_model.pth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Test - Accuracy: 0.552, Precision: 0.30470400000000003, Recall: 0.552, F1-Score: 0.392659793814433\nEpoch 1, Loss: 0.9739934826890627, F1 Score: 0.4464864515894175\nEpoch 2, Loss: 0.9144586796561877, F1 Score: 0.48615408600753535\nEpoch 3, Loss: 0.8352963477373123, F1 Score: 0.5982742597691155\nEpoch 4, Loss: 0.6690496106942495, F1 Score: 0.7089612755545032\nEpoch 5, Loss: 0.5227077280481657, F1 Score: 0.7713850612478728\nEpoch 6, Loss: 0.37999746575951576, F1 Score: 0.8262018389810438\nEpoch 7, Loss: 0.2986296160767476, F1 Score: 0.8821333461859778\nEpoch 8, Loss: 0.22801320627331734, F1 Score: 0.9202210750158099\nEpoch 9, Loss: 0.14270742796361446, F1 Score: 0.9618476024079696\nEpoch 10, Loss: 0.08368351760630806, F1 Score: 0.9863758230074019\nModel saved to /kaggle/working/models/ColBERT_model.pth\nTest - Accuracy: 0.708, Precision: 0.6730457352171638, Recall: 0.708, F1-Score: 0.6881130361648445\nEpoch 1, Loss: 0.978992094596227, F1 Score: 0.46778994976737287\nEpoch 2, Loss: 0.9315755193432173, F1 Score: 0.4637808224689154\nEpoch 3, Loss: 0.7875616525610288, F1 Score: 0.6504064927697314\nEpoch 4, Loss: 0.616013664752245, F1 Score: 0.7389347747747749\nEpoch 5, Loss: 0.49431511387228966, F1 Score: 0.7876029482326035\nEpoch 6, Loss: 0.39930178597569466, F1 Score: 0.8290758057935537\nEpoch 7, Loss: 0.33678811353941757, F1 Score: 0.8618988156890024\nEpoch 8, Loss: 0.25959519669413567, F1 Score: 0.9005974917353202\nEpoch 9, Loss: 0.2126727101082603, F1 Score: 0.9259368522758517\nEpoch 10, Loss: 0.15902335025991002, F1 Score: 0.9518700526184193\nModel saved to /kaggle/working/models/LinkBERT_model.pth\nTest - Accuracy: 0.732, Precision: 0.6854685270425778, Recall: 0.732, F1-Score: 0.7046607058962641\nEpoch 1, Loss: 1.0528821895519893, F1 Score: 0.4388640756390411\nEpoch 2, Loss: 0.8268821015954018, F1 Score: 0.6114591406691735\nEpoch 3, Loss: 0.5813643746078014, F1 Score: 0.7531821943397792\nEpoch 4, Loss: 0.46825906820595264, F1 Score: 0.8091605157701791\nEpoch 5, Loss: 0.36237570581336814, F1 Score: 0.8567915057983784\nEpoch 6, Loss: 0.28540822739402455, F1 Score: 0.9027860959715008\nEpoch 7, Loss: 0.22934265776226917, F1 Score: 0.9286102298864154\nEpoch 8, Loss: 0.17711629392579198, F1 Score: 0.9419248514134452\nEpoch 9, Loss: 0.12015587650239468, F1 Score: 0.9692536059908459\nEpoch 10, Loss: 0.08837895134153466, F1 Score: 0.978270963917865\nModel saved to /kaggle/working/models/BiomedNLP_model.pth\nTest - Accuracy: 0.776, Precision: 0.7676666666666666, Recall: 0.776, F1-Score: 0.7713577416794131\nEpoch 1, Loss: 0.9761907358964285, F1 Score: 0.4317858261206647\nEpoch 2, Loss: 0.855711484948794, F1 Score: 0.5846508485168044\nEpoch 3, Loss: 0.6507169703642527, F1 Score: 0.7321218134215242\nEpoch 4, Loss: 0.5407566217084726, F1 Score: 0.7857791555875695\nEpoch 5, Loss: 0.4618551457921664, F1 Score: 0.8121970546132535\nEpoch 6, Loss: 0.3583769667893648, F1 Score: 0.8634587279551774\nEpoch 7, Loss: 0.2972159792358677, F1 Score: 0.8986831784506077\nEpoch 8, Loss: 0.25973047750691575, F1 Score: 0.9194953370160479\nEpoch 9, Loss: 0.21115423506125808, F1 Score: 0.9394219919724528\nEpoch 10, Loss: 0.15550935035571456, F1 Score: 0.9638106446382281\nModel saved to /kaggle/working/models/BioLinkBERT_model.pth\nTest - Accuracy: 0.804, Precision: 0.8096023671385474, Recall: 0.804, F1-Score: 0.8065231239450619\n","output_type":"stream"}]},{"cell_type":"code","source":"result_context_adamw= result_convert(trainer_context_adamw.results)\nresult_context_adamw","metadata":{"execution":{"iopub.status.busy":"2024-08-06T02:03:22.279943Z","iopub.execute_input":"2024-08-06T02:03:22.280257Z","iopub.status.idle":"2024-08-06T02:03:22.294320Z","shell.execute_reply.started":"2024-08-06T02:03:22.280233Z","shell.execute_reply":"2024-08-06T02:03:22.293547Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"         Model  Accuracy  Precision  Recall  F1 Score\n0         BERT     0.728   0.723585   0.728  0.725729\n1          GPT     0.552   0.304704   0.552  0.392660\n2      ColBERT     0.708   0.673046   0.708  0.688113\n3     LinkBERT     0.732   0.685469   0.732  0.704661\n4    BiomedNLP     0.776   0.767667   0.776  0.771358\n5  BioLinkBERT     0.804   0.809602   0.804  0.806523","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BERT</td>\n      <td>0.728</td>\n      <td>0.723585</td>\n      <td>0.728</td>\n      <td>0.725729</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT</td>\n      <td>0.552</td>\n      <td>0.304704</td>\n      <td>0.552</td>\n      <td>0.392660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ColBERT</td>\n      <td>0.708</td>\n      <td>0.673046</td>\n      <td>0.708</td>\n      <td>0.688113</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LinkBERT</td>\n      <td>0.732</td>\n      <td>0.685469</td>\n      <td>0.732</td>\n      <td>0.704661</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BiomedNLP</td>\n      <td>0.776</td>\n      <td>0.767667</td>\n      <td>0.776</td>\n      <td>0.771358</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BioLinkBERT</td>\n      <td>0.804</td>\n      <td>0.809602</td>\n      <td>0.804</td>\n      <td>0.806523</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}